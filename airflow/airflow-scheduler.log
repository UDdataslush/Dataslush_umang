2025-02-04 12:21:24,813 INFO - Loaded executor: SequentialExecutor
2025-02-04 12:21:24,847 INFO - Starting the scheduler
2025-02-04 12:21:24,848 INFO - Processing each file at most -1 times
2025-02-04 12:21:24,854 INFO - Launched DagFileProcessorManager with pid: 102665
2025-02-04 12:21:24,855 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:21:24,858 INFO - Configured default timezone UTC
2025-02-04 12:26:25,053 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:27:53,207 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-04 00:00:00+00:00, run_after=2025-02-05 00:00:00+00:00
2025-02-04 12:27:53,258 INFO - 2 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T06:57:52.383459+00:00 [scheduled]>
2025-02-04 12:27:53,259 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:27:53,259 INFO - DAG A_brewery_data_pipeline has 1/16 running and queued tasks
2025-02-04 12:27:53,260 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T06:57:52.383459+00:00 [scheduled]>
2025-02-04 12:27:53,261 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>, <TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T06:57:52.383459+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:27:53,262 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:27:53,262 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:27:53,263 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T06:57:52.383459+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:27:53,263 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T06:57:52.383459+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:27:53,270 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:28:03,003 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T06:57:52.383459+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:28:06,731 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-03T00:00:00+00:00', try_number=1, map_index=-1)
2025-02-04 12:28:06,733 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T06:57:52.383459+00:00', try_number=1, map_index=-1)
2025-02-04 12:28:06,743 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T06:57:52.383459+00:00, map_index=-1, run_start_date=2025-02-04 06:58:04.927028+00:00, run_end_date=2025-02-04 06:58:06.146163+00:00, run_duration=1.219135, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 06:57:53.260667+00:00, queued_by_job_id=1, pid=104414
2025-02-04 12:28:06,744 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=scheduled__2025-02-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-04 06:57:55.284007+00:00, run_end_date=2025-02-04 06:58:02.433188+00:00, run_duration=7.149181, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 06:57:53.260667+00:00, queued_by_job_id=1, pid=104343
2025-02-04 12:31:25,303 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:33:02,817 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>
2025-02-04 12:33:02,818 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:33:02,818 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>
2025-02-04 12:33:02,820 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:33:02,820 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-03T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:33:02,821 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:33:02,826 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:33:06,210 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-03T00:00:00+00:00', try_number=2, map_index=-1)
2025-02-04 12:33:06,218 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=scheduled__2025-02-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-04 07:03:04.777302+00:00, run_end_date=2025-02-04 07:03:05.653205+00:00, run_duration=0.875903, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:03:02.819011+00:00, queued_by_job_id=1, pid=106559
2025-02-04 12:33:06,400 ERROR - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-03 00:00:00+00:00: scheduled__2025-02-03T00:00:00+00:00, state:running, queued_at: 2025-02-04 06:57:53.200113+00:00. externally triggered: False> failed
2025-02-04 12:33:06,401 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-03 00:00:00+00:00, run_id=scheduled__2025-02-03T00:00:00+00:00, run_start_date=2025-02-04 06:57:53.220502+00:00, run_end_date=2025-02-04 07:03:06.401290+00:00, run_duration=313.180788, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-03 00:00:00+00:00, data_interval_end=2025-02-04 00:00:00+00:00, dag_hash=b1cc5dbad6f386acf4418cee5270169c
2025-02-04 12:33:06,406 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-04 00:00:00+00:00, run_after=2025-02-05 00:00:00+00:00
2025-02-04 12:33:06,425 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T06:57:52.383459+00:00 [scheduled]>
2025-02-04 12:33:06,426 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:33:06,426 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T06:57:52.383459+00:00 [scheduled]>
2025-02-04 12:33:06,428 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T06:57:52.383459+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:33:06,428 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T06:57:52.383459+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:33:06,429 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T06:57:52.383459+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:33:06,435 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T06:57:52.383459+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:33:09,794 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T06:57:52.383459+00:00', try_number=2, map_index=-1)
2025-02-04 12:33:09,798 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T06:57:52.383459+00:00, map_index=-1, run_start_date=2025-02-04 07:03:08.356693+00:00, run_end_date=2025-02-04 07:03:09.235872+00:00, run_duration=0.879179, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:03:06.427380+00:00, queued_by_job_id=1, pid=106601
2025-02-04 12:33:09,938 ERROR - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 06:57:52.383459+00:00: manual__2025-02-04T06:57:52.383459+00:00, state:running, queued_at: 2025-02-04 06:57:52.404253+00:00. externally triggered: True> failed
2025-02-04 12:33:09,939 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 06:57:52.383459+00:00, run_id=manual__2025-02-04T06:57:52.383459+00:00, run_start_date=2025-02-04 06:57:53.220951+00:00, run_end_date=2025-02-04 07:03:09.938995+00:00, run_duration=316.718044, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-03 06:57:52.383459+00:00, data_interval_end=2025-02-04 06:57:52.383459+00:00, dag_hash=b1cc5dbad6f386acf4418cee5270169c
2025-02-04 12:34:18,413 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:17.163342+00:00 [scheduled]>
2025-02-04 12:34:18,414 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:34:18,414 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:17.163342+00:00 [scheduled]>
2025-02-04 12:34:18,415 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:17.163342+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:34:18,416 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:17.163342+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:34:18,416 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:17.163342+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:34:18,422 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:17.163342+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:34:22,053 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:17.163342+00:00', try_number=1, map_index=-1)
2025-02-04 12:34:22,057 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:04:17.163342+00:00, map_index=-1, run_start_date=2025-02-04 07:04:20.740016+00:00, run_end_date=2025-02-04 07:04:21.540524+00:00, run_duration=0.800508, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:04:18.415082+00:00, queued_by_job_id=1, pid=106781
2025-02-04 12:34:54,945 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:53.250898+00:00 [scheduled]>
2025-02-04 12:34:54,946 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:34:54,946 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:53.250898+00:00 [scheduled]>
2025-02-04 12:34:54,947 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:53.250898+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:34:54,948 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:53.250898+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:34:54,948 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:53.250898+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:34:54,953 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:53.250898+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:34:58,484 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:53.250898+00:00', try_number=1, map_index=-1)
2025-02-04 12:34:58,488 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:04:53.250898+00:00, map_index=-1, run_start_date=2025-02-04 07:04:56.982931+00:00, run_end_date=2025-02-04 07:04:57.949659+00:00, run_duration=0.966728, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:04:54.947053+00:00, queued_by_job_id=1, pid=107052
2025-02-04 12:36:25,363 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:39:22,136 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:17.163342+00:00 [scheduled]>
2025-02-04 12:39:22,136 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:39:22,136 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:17.163342+00:00 [scheduled]>
2025-02-04 12:39:22,138 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:17.163342+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:39:22,139 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:17.163342+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:39:22,139 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:17.163342+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:39:22,145 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:17.163342+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:39:25,504 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:17.163342+00:00', try_number=2, map_index=-1)
2025-02-04 12:39:25,508 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:04:17.163342+00:00, map_index=-1, run_start_date=2025-02-04 07:09:24.131945+00:00, run_end_date=2025-02-04 07:09:24.978496+00:00, run_duration=0.846551, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:09:22.137540+00:00, queued_by_job_id=1, pid=108242
2025-02-04 12:39:25,677 ERROR - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 07:04:17.163342+00:00: manual__2025-02-04T07:04:17.163342+00:00, state:running, queued_at: 2025-02-04 07:04:17.178128+00:00. externally triggered: True> failed
2025-02-04 12:39:25,677 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 07:04:17.163342+00:00, run_id=manual__2025-02-04T07:04:17.163342+00:00, run_start_date=2025-02-04 07:04:18.384827+00:00, run_end_date=2025-02-04 07:09:25.677522+00:00, run_duration=307.292695, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-03 07:04:17.163342+00:00, data_interval_end=2025-02-04 07:04:17.163342+00:00, dag_hash=b1cc5dbad6f386acf4418cee5270169c
2025-02-04 12:39:58,821 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:53.250898+00:00 [scheduled]>
2025-02-04 12:39:58,821 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:39:58,822 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:53.250898+00:00 [scheduled]>
2025-02-04 12:39:58,823 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:04:53.250898+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:39:58,823 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:53.250898+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:39:58,824 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:53.250898+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:39:58,830 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:04:53.250898+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:40:02,105 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:04:53.250898+00:00', try_number=2, map_index=-1)
2025-02-04 12:40:02,109 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:04:53.250898+00:00, map_index=-1, run_start_date=2025-02-04 07:10:00.702519+00:00, run_end_date=2025-02-04 07:10:01.548905+00:00, run_duration=0.846386, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:09:58.822654+00:00, queued_by_job_id=1, pid=108290
2025-02-04 12:40:02,302 ERROR - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 07:04:53.250898+00:00: manual__2025-02-04T07:04:53.250898+00:00, state:running, queued_at: 2025-02-04 07:04:53.261824+00:00. externally triggered: True> failed
2025-02-04 12:40:02,303 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 07:04:53.250898+00:00, run_id=manual__2025-02-04T07:04:53.250898+00:00, run_start_date=2025-02-04 07:04:54.919184+00:00, run_end_date=2025-02-04 07:10:02.303659+00:00, run_duration=307.384475, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-03 07:04:53.250898+00:00, data_interval_end=2025-02-04 07:04:53.250898+00:00, dag_hash=b1cc5dbad6f386acf4418cee5270169c
2025-02-04 12:41:25,557 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:43:06,178 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [scheduled]>
2025-02-04 12:43:06,179 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:43:06,179 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [scheduled]>
2025-02-04 12:43:06,181 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:43:06,182 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:13:04.631466+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:43:06,182 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:13:04.631466+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:43:06,189 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:13:04.631466+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:43:08,646 ERROR - Failed to execute task Command '['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:13:04.631466+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']' returned non-zero exit status 1..
2025-02-04 12:43:08,647 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:13:04.631466+00:00', try_number=1, map_index=-1)
2025-02-04 12:43:08,650 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:13:04.631466+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor=SequentialExecutor(parallelism=32), executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:13:06.180329+00:00, queued_by_job_id=1, pid=None
2025-02-04 12:43:08,651 ERROR - Executor SequentialExecutor(parallelism=32) reported that the task instance <TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
2025-02-04 12:43:08,662 ERROR - Executor SequentialExecutor(parallelism=32) reported that the task instance <TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
2025-02-04 12:43:08,671 INFO - Marking task as UP_FOR_RETRY. dag_id=A_brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:13:04.631466+00:00, execution_date=20250204T071304, start_date=, end_date=20250204T071308
2025-02-04 12:46:20,694 INFO - Setting next_dagrun for brewery_data_pipeline to 2025-02-04 00:00:00+00:00, run_after=2025-02-05 00:00:00+00:00
2025-02-04 12:46:20,737 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>
2025-02-04 12:46:20,738 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:46:20,738 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>
2025-02-04 12:46:20,740 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:46:20,740 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:46:20,741 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:46:20,746 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:46:44,344 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-03T00:00:00+00:00', try_number=1, map_index=-1)
2025-02-04 12:46:44,348 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=scheduled__2025-02-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-04 07:16:22.973995+00:00, run_end_date=2025-02-04 07:16:43.778490+00:00, run_duration=20.804495, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:16:20.739220+00:00, queued_by_job_id=1, pid=109890
2025-02-04 12:46:44,374 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:46:44,676 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-03 00:00:00+00:00: scheduled__2025-02-03T00:00:00+00:00, state:running, queued_at: 2025-02-04 07:16:20.688077+00:00. externally triggered: False> successful
2025-02-04 12:46:44,676 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-03 00:00:00+00:00, run_id=scheduled__2025-02-03T00:00:00+00:00, run_start_date=2025-02-04 07:16:20.705483+00:00, run_end_date=2025-02-04 07:16:44.676653+00:00, run_duration=23.97117, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-03 00:00:00+00:00, data_interval_end=2025-02-04 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-04 12:46:44,681 INFO - Setting next_dagrun for brewery_data_pipeline to 2025-02-04 00:00:00+00:00, run_after=2025-02-05 00:00:00+00:00
2025-02-04 12:46:44,694 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:16:20.316903+00:00 [scheduled]>
2025-02-04 12:46:44,695 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-04 12:46:44,695 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:16:20.316903+00:00 [scheduled]>
2025-02-04 12:46:44,696 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:16:20.316903+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-04 12:46:44,697 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:16:20.316903+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-04 12:46:44,697 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:16:20.316903+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:46:44,703 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-04T07:16:20.316903+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-04 12:46:48,809 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-04T07:16:20.316903+00:00', try_number=1, map_index=-1)
2025-02-04 12:46:48,817 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-04T07:16:20.316903+00:00, map_index=-1, run_start_date=2025-02-04 07:16:47.293688+00:00, run_end_date=2025-02-04 07:16:48.222047+00:00, run_duration=0.928359, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-04 07:16:44.696053+00:00, queued_by_job_id=1, pid=110058
2025-02-04 12:46:48,975 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-04 07:16:20.316903+00:00: manual__2025-02-04T07:16:20.316903+00:00, state:running, queued_at: 2025-02-04 07:16:20.374330+00:00. externally triggered: True> successful
2025-02-04 12:46:48,975 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-04 07:16:20.316903+00:00, run_id=manual__2025-02-04T07:16:20.316903+00:00, run_start_date=2025-02-04 07:16:44.658099+00:00, run_end_date=2025-02-04 07:16:48.975499+00:00, run_duration=4.3174, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-03 00:00:00+00:00, data_interval_end=2025-02-04 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-04 12:51:44,568 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 12:56:44,742 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:01:44,927 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:06:45,020 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:11:45,224 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:16:45,266 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:21:45,414 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:26:45,565 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:31:45,653 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:36:45,801 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:41:46,051 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:46:46,216 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:51:46,361 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 13:56:46,541 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:01:46,711 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:06:46,728 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:11:46,879 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:16:47,048 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:21:47,221 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:26:47,366 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:31:47,642 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:36:48,014 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:41:48,165 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:46:48,317 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:51:48,477 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 14:56:48,672 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:01:48,846 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:06:48,989 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:11:49,146 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:16:49,343 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:21:49,499 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:26:49,647 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:31:49,673 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:36:49,829 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:41:49,979 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-04 15:46:06,952 INFO - Exiting gracefully upon receiving signal 15
2025-02-04 15:46:07,300 INFO - Sending 15 to group 102665. PIDs of all processes in the group: []
2025-02-04 15:46:07,301 INFO - Sending the signal 15 to group 102665
2025-02-04 15:46:07,301 INFO - Sending the signal 15 to process 102665 as process group is missing.
2025-02-04 15:46:07,303 INFO - Sending 15 to group 102665. PIDs of all processes in the group: []
2025-02-04 15:46:07,303 INFO - Sending the signal 15 to group 102665
2025-02-04 15:46:07,304 INFO - Sending the signal 15 to process 102665 as process group is missing.
2025-02-04 15:46:07,304 INFO - Exited execute loop
2025-02-05 10:49:38,176 INFO - Loaded executor: SequentialExecutor
2025-02-05 10:49:38,216 INFO - Starting the scheduler
2025-02-05 10:49:38,217 INFO - Processing each file at most -1 times
2025-02-05 10:49:38,223 INFO - Launched DagFileProcessorManager with pid: 254562
2025-02-05 10:49:38,225 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 10:49:38,227 INFO - Configured default timezone UTC
2025-02-05 10:49:38,550 INFO - Setting next_dagrun for brewery_data_pipeline to 2025-02-05 00:00:00+00:00, run_after=2025-02-06 00:00:00+00:00
2025-02-05 10:49:38,609 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-04T00:00:00+00:00 [scheduled]>
2025-02-05 10:49:38,610 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 10:49:38,610 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-04T00:00:00+00:00 [scheduled]>
2025-02-05 10:49:38,612 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 10:49:38,612 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-04T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 10:49:38,613 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:49:38,620 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:49:41,769 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-04T00:00:00+00:00', try_number=1, map_index=-1)
2025-02-05 10:49:41,776 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=scheduled__2025-02-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-05 05:19:40.381132+00:00, run_end_date=2025-02-05 05:19:41.273705+00:00, run_duration=0.892573, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:19:38.611133+00:00, queued_by_job_id=12, pid=254574
2025-02-05 10:51:19,134 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:21:17.796069+00:00 [scheduled]>
2025-02-05 10:51:19,135 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 10:51:19,135 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:21:17.796069+00:00 [scheduled]>
2025-02-05 10:51:19,137 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:21:17.796069+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 10:51:19,138 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:21:17.796069+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 10:51:19,138 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:21:17.796069+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:51:19,144 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:21:17.796069+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:51:22,490 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:21:17.796069+00:00', try_number=1, map_index=-1)
2025-02-05 10:51:22,494 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:21:17.796069+00:00, map_index=-1, run_start_date=2025-02-05 05:21:21.176652+00:00, run_end_date=2025-02-05 05:21:21.948502+00:00, run_duration=0.77185, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:21:19.136586+00:00, queued_by_job_id=12, pid=254998
2025-02-05 10:54:38,418 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 10:54:41,365 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-04T00:00:00+00:00 [scheduled]>
2025-02-05 10:54:41,365 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 10:54:41,365 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-04T00:00:00+00:00 [scheduled]>
2025-02-05 10:54:41,367 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 10:54:41,367 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-04T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 10:54:41,367 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:54:41,373 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:54:44,778 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-04T00:00:00+00:00', try_number=2, map_index=-1)
2025-02-05 10:54:44,782 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=scheduled__2025-02-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-05 05:24:43.538700+00:00, run_end_date=2025-02-05 05:24:44.251741+00:00, run_duration=0.713041, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:24:41.366312+00:00, queued_by_job_id=12, pid=256416
2025-02-05 10:54:44,958 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-04 00:00:00+00:00: scheduled__2025-02-04T00:00:00+00:00, state:running, queued_at: 2025-02-05 05:19:38.536191+00:00. externally triggered: False> failed
2025-02-05 10:54:44,959 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-04 00:00:00+00:00, run_id=scheduled__2025-02-04T00:00:00+00:00, run_start_date=2025-02-05 05:19:38.566558+00:00, run_end_date=2025-02-05 05:24:44.958976+00:00, run_duration=306.392418, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 10:54:44,963 INFO - Setting next_dagrun for brewery_data_pipeline to 2025-02-05 00:00:00+00:00, run_after=2025-02-06 00:00:00+00:00
2025-02-05 10:56:22,840 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:21:17.796069+00:00 [scheduled]>
2025-02-05 10:56:22,841 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 10:56:22,842 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:21:17.796069+00:00 [scheduled]>
2025-02-05 10:56:22,844 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:21:17.796069+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 10:56:22,844 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:21:17.796069+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 10:56:22,845 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:21:17.796069+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:56:22,852 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:21:17.796069+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:56:25,753 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:21:17.796069+00:00', try_number=2, map_index=-1)
2025-02-05 10:56:25,756 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:21:17.796069+00:00, map_index=-1, run_start_date=2025-02-05 05:26:24.604558+00:00, run_end_date=2025-02-05 05:26:25.256495+00:00, run_duration=0.651937, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:26:22.842921+00:00, queued_by_job_id=12, pid=256601
2025-02-05 10:56:26,035 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 05:21:17.796069+00:00: manual__2025-02-05T05:21:17.796069+00:00, state:running, queued_at: 2025-02-05 05:21:17.808699+00:00. externally triggered: True> failed
2025-02-05 10:56:26,035 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 05:21:17.796069+00:00, run_id=manual__2025-02-05T05:21:17.796069+00:00, run_start_date=2025-02-05 05:21:19.077693+00:00, run_end_date=2025-02-05 05:26:26.035657+00:00, run_duration=306.957964, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 10:58:03,295 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:28:01.879037+00:00 [scheduled]>
2025-02-05 10:58:03,296 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 10:58:03,297 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:28:01.879037+00:00 [scheduled]>
2025-02-05 10:58:03,300 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:28:01.879037+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 10:58:03,300 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:28:01.879037+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 10:58:03,301 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:28:01.879037+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:58:03,308 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:28:01.879037+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 10:58:06,767 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:28:01.879037+00:00', try_number=1, map_index=-1)
2025-02-05 10:58:06,772 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:28:01.879037+00:00, map_index=-1, run_start_date=2025-02-05 05:28:05.272804+00:00, run_end_date=2025-02-05 05:28:06.177926+00:00, run_duration=0.905122, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:28:03.298473+00:00, queued_by_job_id=12, pid=256814
2025-02-05 10:59:38,583 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:03:07,318 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:28:01.879037+00:00 [scheduled]>
2025-02-05 11:03:07,318 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:03:07,319 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:28:01.879037+00:00 [scheduled]>
2025-02-05 11:03:07,320 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:28:01.879037+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:03:07,320 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:28:01.879037+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:03:07,320 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:28:01.879037+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:03:07,327 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:28:01.879037+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:03:20,925 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:28:01.879037+00:00', try_number=2, map_index=-1)
2025-02-05 11:03:20,929 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:28:01.879037+00:00, map_index=-1, run_start_date=2025-02-05 05:33:09.368666+00:00, run_end_date=2025-02-05 05:33:20.330844+00:00, run_duration=10.962178, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:33:07.319611+00:00, queued_by_job_id=12, pid=258971
2025-02-05 11:03:21,162 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 05:28:01.879037+00:00: manual__2025-02-05T05:28:01.879037+00:00, state:running, queued_at: 2025-02-05 05:28:01.895943+00:00. externally triggered: True> failed
2025-02-05 11:03:21,163 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 05:28:01.879037+00:00, run_id=manual__2025-02-05T05:28:01.879037+00:00, run_start_date=2025-02-05 05:28:03.245128+00:00, run_end_date=2025-02-05 05:33:21.163448+00:00, run_duration=317.91832, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 11:04:38,777 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:09:38,924 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:11:14,135 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>
2025-02-05 11:11:14,136 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:11:14,136 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>
2025-02-05 11:11:14,138 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:11:14,139 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:41:13.872494+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:11:14,140 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:41:13.872494+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:11:14,150 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:41:13.872494+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:11:17,825 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:41:13.872494+00:00', try_number=1, map_index=-1)
2025-02-05 11:11:17,829 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:41:13.872494+00:00, map_index=-1, run_start_date=2025-02-05 05:41:16.207102+00:00, run_end_date=2025-02-05 05:41:17.289372+00:00, run_duration=1.08227, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:41:14.137707+00:00, queued_by_job_id=12, pid=260167
2025-02-05 11:14:39,100 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:16:17,567 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>
2025-02-05 11:16:17,567 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:16:17,567 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>
2025-02-05 11:16:17,569 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:16:17,569 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:41:13.872494+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:16:17,569 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:41:13.872494+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:16:17,576 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:41:13.872494+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:16:20,965 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:41:13.872494+00:00', try_number=2, map_index=-1)
2025-02-05 11:16:20,969 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:41:13.872494+00:00, map_index=-1, run_start_date=2025-02-05 05:46:19.526134+00:00, run_end_date=2025-02-05 05:46:20.391911+00:00, run_duration=0.865777, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:46:17.568349+00:00, queued_by_job_id=12, pid=261592
2025-02-05 11:19:39,300 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:21:20,662 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>
2025-02-05 11:21:20,662 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:21:20,663 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>
2025-02-05 11:21:20,664 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:41:13.872494+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:21:20,664 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:41:13.872494+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:21:20,664 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:41:13.872494+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:21:20,671 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:41:13.872494+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:21:34,136 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:41:13.872494+00:00', try_number=3, map_index=-1)
2025-02-05 11:21:34,141 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:41:13.872494+00:00, map_index=-1, run_start_date=2025-02-05 05:51:22.728748+00:00, run_end_date=2025-02-05 05:51:33.573534+00:00, run_duration=10.844786, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:51:20.663575+00:00, queued_by_job_id=12, pid=263281
2025-02-05 11:21:34,539 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 05:41:13.872494+00:00: manual__2025-02-05T05:41:13.872494+00:00, state:running, queued_at: 2025-02-05 05:41:13.884499+00:00. externally triggered: True> failed
2025-02-05 11:21:34,539 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 05:41:13.872494+00:00, run_id=manual__2025-02-05T05:41:13.872494+00:00, run_start_date=2025-02-05 05:41:14.094936+00:00, run_end_date=2025-02-05 05:51:34.539543+00:00, run_duration=620.444607, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=1525d18a92b2e88fc46eddbc7b40fbc3
2025-02-05 11:24:28,075 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>
2025-02-05 11:24:28,076 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:24:28,076 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>
2025-02-05 11:24:28,077 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:24:28,078 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:54:27+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:24:28,078 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:54:27+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:24:28,085 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:54:27+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:24:31,181 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:54:27+00:00', try_number=1, map_index=-1)
2025-02-05 11:24:31,185 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:54:27+00:00, map_index=-1, run_start_date=2025-02-05 05:54:29.870397+00:00, run_end_date=2025-02-05 05:54:30.661907+00:00, run_duration=0.79151, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:54:28.077125+00:00, queued_by_job_id=12, pid=263754
2025-02-05 11:24:39,481 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:29:31,451 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>
2025-02-05 11:29:31,452 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:29:31,453 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>
2025-02-05 11:29:31,455 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:29:31,456 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:54:27+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:29:31,457 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:54:27+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:29:31,464 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:54:27+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:29:34,940 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:54:27+00:00', try_number=2, map_index=-1)
2025-02-05 11:29:34,944 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:54:27+00:00, map_index=-1, run_start_date=2025-02-05 05:59:33.523804+00:00, run_end_date=2025-02-05 05:59:34.394205+00:00, run_duration=0.870401, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:59:31.454161+00:00, queued_by_job_id=12, pid=264416
2025-02-05 11:29:39,655 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:29:50,355 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>
2025-02-05 11:29:50,356 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:29:50,356 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>
2025-02-05 11:29:50,357 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:29:50,358 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:49.122984+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:29:50,358 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:49.122984+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:29:50,365 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:49.122984+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:29:54,050 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:49.122984+00:00', try_number=1, map_index=-1)
2025-02-05 11:29:54,054 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:59:49.122984+00:00, map_index=-1, run_start_date=2025-02-05 05:59:52.733781+00:00, run_end_date=2025-02-05 05:59:53.485165+00:00, run_duration=0.751384, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:59:50.357129+00:00, queued_by_job_id=12, pid=264600
2025-02-05 11:29:54,237 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>
2025-02-05 11:29:54,238 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:29:54,238 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>
2025-02-05 11:29:54,239 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:29:54,240 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:53.771782+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:29:54,240 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:53.771782+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:29:54,247 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:53.771782+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:29:57,627 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:53.771782+00:00', try_number=1, map_index=-1)
2025-02-05 11:29:57,631 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:59:53.771782+00:00, map_index=-1, run_start_date=2025-02-05 05:59:56.247674+00:00, run_end_date=2025-02-05 05:59:57.050949+00:00, run_duration=0.803275, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 05:59:54.239153+00:00, queued_by_job_id=12, pid=264627
2025-02-05 11:34:34,615 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>
2025-02-05 11:34:34,615 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:34:34,615 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>
2025-02-05 11:34:34,617 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:54:27+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:34:34,617 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:54:27+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:34:34,617 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:54:27+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:34:34,625 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:54:27+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:34:44,171 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:54:27+00:00', try_number=3, map_index=-1)
2025-02-05 11:34:44,175 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:54:27+00:00, map_index=-1, run_start_date=2025-02-05 06:04:36.678613+00:00, run_end_date=2025-02-05 06:04:43.625412+00:00, run_duration=6.946799, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:04:34.616376+00:00, queued_by_job_id=12, pid=266602
2025-02-05 11:34:44,206 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:34:44,512 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 05:54:27+00:00: manual__2025-02-05T05:54:27+00:00, state:running, queued_at: 2025-02-05 05:54:27.462680+00:00. externally triggered: True> failed
2025-02-05 11:34:44,513 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 05:54:27+00:00, run_id=manual__2025-02-05T05:54:27+00:00, run_start_date=2025-02-05 05:54:28.041600+00:00, run_end_date=2025-02-05 06:04:44.513463+00:00, run_duration=616.471863, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=1525d18a92b2e88fc46eddbc7b40fbc3
2025-02-05 11:34:53,752 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>
2025-02-05 11:34:53,752 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:34:53,753 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>
2025-02-05 11:34:53,754 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:34:53,755 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:49.122984+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:34:53,755 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:49.122984+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:34:53,761 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:49.122984+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:34:57,186 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:49.122984+00:00', try_number=2, map_index=-1)
2025-02-05 11:34:57,190 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:59:49.122984+00:00, map_index=-1, run_start_date=2025-02-05 06:04:55.769810+00:00, run_end_date=2025-02-05 06:04:56.624440+00:00, run_duration=0.85463, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:04:53.753816+00:00, queued_by_job_id=12, pid=266745
2025-02-05 11:34:57,412 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>
2025-02-05 11:34:57,413 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:34:57,413 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>
2025-02-05 11:34:57,414 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:34:57,415 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:53.771782+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:34:57,415 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:53.771782+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:34:57,422 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:53.771782+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:35:00,663 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:53.771782+00:00', try_number=2, map_index=-1)
2025-02-05 11:35:00,667 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:59:53.771782+00:00, map_index=-1, run_start_date=2025-02-05 06:04:59.396034+00:00, run_end_date=2025-02-05 06:05:00.111687+00:00, run_duration=0.715653, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:04:57.414065+00:00, queued_by_job_id=12, pid=266787
2025-02-05 11:39:44,490 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:39:57,287 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>
2025-02-05 11:39:57,287 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:39:57,287 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>
2025-02-05 11:39:57,289 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:49.122984+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:39:57,289 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:49.122984+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:39:57,290 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:49.122984+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:39:57,296 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:49.122984+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:40:25,117 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:49.122984+00:00', try_number=3, map_index=-1)
2025-02-05 11:40:25,121 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:59:49.122984+00:00, map_index=-1, run_start_date=2025-02-05 06:09:59.393057+00:00, run_end_date=2025-02-05 06:10:24.559454+00:00, run_duration=25.166397, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:09:57.288472+00:00, queued_by_job_id=12, pid=268709
2025-02-05 11:40:25,142 INFO - Heartbeat recovered after 31.52 seconds
2025-02-05 11:40:25,362 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 05:59:49.122984+00:00: manual__2025-02-05T05:59:49.122984+00:00, state:running, queued_at: 2025-02-05 05:59:49.139744+00:00. externally triggered: True> failed
2025-02-05 11:40:25,363 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 05:59:49.122984+00:00, run_id=manual__2025-02-05T05:59:49.122984+00:00, run_start_date=2025-02-05 05:59:50.323781+00:00, run_end_date=2025-02-05 06:10:25.363174+00:00, run_duration=635.039393, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=1525d18a92b2e88fc46eddbc7b40fbc3
2025-02-05 11:40:25,378 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>
2025-02-05 11:40:25,378 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:40:25,379 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>
2025-02-05 11:40:25,380 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T05:59:53.771782+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:40:25,381 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:53.771782+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:40:25,381 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:53.771782+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:40:25,388 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T05:59:53.771782+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:40:28,971 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T05:59:53.771782+00:00', try_number=3, map_index=-1)
2025-02-05 11:40:28,975 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T05:59:53.771782+00:00, map_index=-1, run_start_date=2025-02-05 06:10:27.458709+00:00, run_end_date=2025-02-05 06:10:28.375619+00:00, run_duration=0.91691, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=30, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:10:25.379723+00:00, queued_by_job_id=12, pid=268906
2025-02-05 11:40:29,128 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 05:59:53.771782+00:00: manual__2025-02-05T05:59:53.771782+00:00, state:running, queued_at: 2025-02-05 05:59:53.785459+00:00. externally triggered: True> failed
2025-02-05 11:40:29,129 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 05:59:53.771782+00:00, run_id=manual__2025-02-05T05:59:53.771782+00:00, run_start_date=2025-02-05 05:59:54.206788+00:00, run_end_date=2025-02-05 06:10:29.129580+00:00, run_duration=634.922792, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=1525d18a92b2e88fc46eddbc7b40fbc3
2025-02-05 11:43:47,118 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:13:45.871936+00:00 [scheduled]>
2025-02-05 11:43:47,119 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:43:47,119 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:13:45.871936+00:00 [scheduled]>
2025-02-05 11:43:47,120 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:13:45.871936+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:43:47,121 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:13:45.871936+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:43:47,121 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:13:45.871936+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:43:47,127 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:13:45.871936+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:43:50,539 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:13:45.871936+00:00', try_number=1, map_index=-1)
2025-02-05 11:43:50,544 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:13:45.871936+00:00, map_index=-1, run_start_date=2025-02-05 06:13:49.178952+00:00, run_end_date=2025-02-05 06:13:49.955830+00:00, run_duration=0.776878, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:13:47.119868+00:00, queued_by_job_id=12, pid=269583
2025-02-05 11:44:44,653 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:48:50,318 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:13:45.871936+00:00 [scheduled]>
2025-02-05 11:48:50,318 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:48:50,319 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:13:45.871936+00:00 [scheduled]>
2025-02-05 11:48:50,321 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:13:45.871936+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:48:50,322 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:13:45.871936+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:48:50,322 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:13:45.871936+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:48:50,330 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:13:45.871936+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:48:53,792 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:13:45.871936+00:00', try_number=2, map_index=-1)
2025-02-05 11:48:53,796 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:13:45.871936+00:00, map_index=-1, run_start_date=2025-02-05 06:18:52.410702+00:00, run_end_date=2025-02-05 06:18:53.244309+00:00, run_duration=0.833607, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:18:50.320318+00:00, queued_by_job_id=12, pid=271175
2025-02-05 11:48:53,983 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:13:45.871936+00:00: manual__2025-02-05T06:13:45.871936+00:00, state:running, queued_at: 2025-02-05 06:13:45.883752+00:00. externally triggered: True> failed
2025-02-05 11:48:53,984 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:13:45.871936+00:00, run_id=manual__2025-02-05T06:13:45.871936+00:00, run_start_date=2025-02-05 06:13:47.094936+00:00, run_end_date=2025-02-05 06:18:53.983894+00:00, run_duration=306.888958, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 11:49:44,972 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:50:45,398 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:20:44+00:00 [scheduled]>
2025-02-05 11:50:45,398 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:50:45,398 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:20:44+00:00 [scheduled]>
2025-02-05 11:50:45,399 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:20:44+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:50:45,400 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:20:44+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:50:45,400 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:20:44+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:50:45,407 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:20:44+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:50:48,686 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:20:44+00:00', try_number=1, map_index=-1)
2025-02-05 11:50:48,690 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:20:44+00:00, map_index=-1, run_start_date=2025-02-05 06:20:47.233083+00:00, run_end_date=2025-02-05 06:20:48.131686+00:00, run_duration=0.898603, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:20:45.399350+00:00, queued_by_job_id=12, pid=271732
2025-02-05 11:51:22,298 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:21:21+00:00 [scheduled]>
2025-02-05 11:51:22,299 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:51:22,299 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:21:21+00:00 [scheduled]>
2025-02-05 11:51:22,301 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:21:21+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:51:22,302 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:21:21+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:51:22,302 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:21:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:51:22,308 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:21:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:51:25,555 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:21:21+00:00', try_number=1, map_index=-1)
2025-02-05 11:51:25,559 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:21:21+00:00, map_index=-1, run_start_date=2025-02-05 06:21:24.214011+00:00, run_end_date=2025-02-05 06:21:24.998708+00:00, run_duration=0.784697, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:21:22.300432+00:00, queued_by_job_id=12, pid=271895
2025-02-05 11:54:45,122 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 11:54:55,240 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:24:54.876479+00:00 [scheduled]>
2025-02-05 11:54:55,240 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:54:55,240 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:24:54.876479+00:00 [scheduled]>
2025-02-05 11:54:55,242 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:24:54.876479+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:54:55,242 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:24:54.876479+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:54:55,243 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:24:54.876479+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:54:55,249 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:24:54.876479+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:55:05,000 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:24:54.876479+00:00', try_number=1, map_index=-1)
2025-02-05 11:55:05,004 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:24:54.876479+00:00, map_index=-1, run_start_date=2025-02-05 06:24:57.726989+00:00, run_end_date=2025-02-05 06:25:04.448977+00:00, run_duration=6.721988, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:24:55.241317+00:00, queued_by_job_id=12, pid=272545
2025-02-05 11:55:48,947 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:20:44+00:00 [scheduled]>
2025-02-05 11:55:48,948 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:55:48,948 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:20:44+00:00 [scheduled]>
2025-02-05 11:55:48,949 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:20:44+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:55:48,950 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:20:44+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:55:48,950 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:20:44+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:55:48,956 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:20:44+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:55:52,547 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:20:44+00:00', try_number=2, map_index=-1)
2025-02-05 11:55:52,551 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:20:44+00:00, map_index=-1, run_start_date=2025-02-05 06:25:50.938434+00:00, run_end_date=2025-02-05 06:25:52.034092+00:00, run_duration=1.095658, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:25:48.949094+00:00, queued_by_job_id=12, pid=272904
2025-02-05 11:55:52,745 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:20:44+00:00: manual__2025-02-05T06:20:44+00:00, state:running, queued_at: 2025-02-05 06:20:44.644587+00:00. externally triggered: True> failed
2025-02-05 11:55:52,745 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:20:44+00:00, run_id=manual__2025-02-05T06:20:44+00:00, run_start_date=2025-02-05 06:20:45.375757+00:00, run_end_date=2025-02-05 06:25:52.745912+00:00, run_duration=307.370155, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 11:56:25,156 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:21:21+00:00 [scheduled]>
2025-02-05 11:56:25,157 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 11:56:25,157 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:21:21+00:00 [scheduled]>
2025-02-05 11:56:25,158 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:21:21+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 11:56:25,159 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:21:21+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 11:56:25,159 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:21:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:56:25,165 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:21:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 11:56:28,235 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:21:21+00:00', try_number=2, map_index=-1)
2025-02-05 11:56:28,239 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:21:21+00:00, map_index=-1, run_start_date=2025-02-05 06:26:26.968821+00:00, run_end_date=2025-02-05 06:26:27.706927+00:00, run_duration=0.738106, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:26:25.157931+00:00, queued_by_job_id=12, pid=272949
2025-02-05 11:56:28,409 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:21:21+00:00: manual__2025-02-05T06:21:21+00:00, state:running, queued_at: 2025-02-05 06:21:21.812910+00:00. externally triggered: True> failed
2025-02-05 11:56:28,410 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:21:21+00:00, run_id=manual__2025-02-05T06:21:21+00:00, run_start_date=2025-02-05 06:21:22.268456+00:00, run_end_date=2025-02-05 06:26:28.410363+00:00, run_duration=306.141907, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 11:59:45,300 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:00:05,125 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:24:54.876479+00:00 [scheduled]>
2025-02-05 12:00:05,125 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:00:05,125 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:24:54.876479+00:00 [scheduled]>
2025-02-05 12:00:05,127 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:24:54.876479+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:00:05,127 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:24:54.876479+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:00:05,128 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:24:54.876479+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:00:05,134 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:24:54.876479+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:00:08,522 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:24:54.876479+00:00', try_number=2, map_index=-1)
2025-02-05 12:00:08,526 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:24:54.876479+00:00, map_index=-1, run_start_date=2025-02-05 06:30:07.176114+00:00, run_end_date=2025-02-05 06:30:07.966465+00:00, run_duration=0.790351, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:30:05.126399+00:00, queued_by_job_id=12, pid=274460
2025-02-05 12:00:08,784 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:24:54.876479+00:00: manual__2025-02-05T06:24:54.876479+00:00, state:running, queued_at: 2025-02-05 06:24:54.891032+00:00. externally triggered: True> failed
2025-02-05 12:00:08,785 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:24:54.876479+00:00, run_id=manual__2025-02-05T06:24:54.876479+00:00, run_start_date=2025-02-05 06:24:55.207553+00:00, run_end_date=2025-02-05 06:30:08.785148+00:00, run_duration=313.577595, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 12:04:45,497 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:05:20,882 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:35:19.742522+00:00 [scheduled]>
2025-02-05 12:05:20,882 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:05:20,882 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:35:19.742522+00:00 [scheduled]>
2025-02-05 12:05:20,884 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:35:19.742522+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:05:20,884 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:35:19.742522+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:05:20,884 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:35:19.742522+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:05:20,891 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:35:19.742522+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:05:24,363 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:35:19.742522+00:00', try_number=1, map_index=-1)
2025-02-05 12:05:24,367 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:35:19.742522+00:00, map_index=-1, run_start_date=2025-02-05 06:35:22.921233+00:00, run_end_date=2025-02-05 06:35:23.779303+00:00, run_duration=0.85807, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:35:20.883468+00:00, queued_by_job_id=12, pid=276002
2025-02-05 12:09:45,678 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:10:23,805 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:35:19.742522+00:00 [scheduled]>
2025-02-05 12:10:23,805 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:10:23,805 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:35:19.742522+00:00 [scheduled]>
2025-02-05 12:10:23,806 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:35:19.742522+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:10:23,807 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:35:19.742522+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:10:23,807 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:35:19.742522+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:10:23,814 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:35:19.742522+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:10:27,364 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:35:19.742522+00:00', try_number=2, map_index=-1)
2025-02-05 12:10:27,368 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:35:19.742522+00:00, map_index=-1, run_start_date=2025-02-05 06:40:25.944579+00:00, run_end_date=2025-02-05 06:40:26.837185+00:00, run_duration=0.892606, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:40:23.806227+00:00, queued_by_job_id=12, pid=278028
2025-02-05 12:10:27,575 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:35:19.742522+00:00: manual__2025-02-05T06:35:19.742522+00:00, state:running, queued_at: 2025-02-05 06:35:19.753394+00:00. externally triggered: True> failed
2025-02-05 12:10:27,576 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:35:19.742522+00:00, run_id=manual__2025-02-05T06:35:19.742522+00:00, run_start_date=2025-02-05 06:35:20.856292+00:00, run_end_date=2025-02-05 06:40:27.576636+00:00, run_duration=306.720344, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 12:11:55,228 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:41:54+00:00 [scheduled]>
2025-02-05 12:11:55,228 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:11:55,229 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:41:54+00:00 [scheduled]>
2025-02-05 12:11:55,230 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:41:54+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:11:55,231 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:41:54+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:11:55,231 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:41:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:11:55,238 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:41:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:11:58,633 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:41:54+00:00', try_number=1, map_index=-1)
2025-02-05 12:11:58,637 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:41:54+00:00, map_index=-1, run_start_date=2025-02-05 06:41:57.033984+00:00, run_end_date=2025-02-05 06:41:58.101573+00:00, run_duration=1.067589, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:41:55.229819+00:00, queued_by_job_id=12, pid=278548
2025-02-05 12:14:45,851 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:16:58,747 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:41:54+00:00 [scheduled]>
2025-02-05 12:16:58,748 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:16:58,748 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:41:54+00:00 [scheduled]>
2025-02-05 12:16:58,749 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:41:54+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:16:58,750 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:41:54+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:16:58,750 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:41:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:16:58,756 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:41:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:17:08,254 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:41:54+00:00', try_number=2, map_index=-1)
2025-02-05 12:17:08,258 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:41:54+00:00, map_index=-1, run_start_date=2025-02-05 06:47:00.754227+00:00, run_end_date=2025-02-05 06:47:07.709341+00:00, run_duration=6.955114, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:46:58.749008+00:00, queued_by_job_id=12, pid=279010
2025-02-05 12:17:08,461 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:41:54+00:00: manual__2025-02-05T06:41:54+00:00, state:running, queued_at: 2025-02-05 06:41:54.028084+00:00. externally triggered: True> failed
2025-02-05 12:17:08,461 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:41:54+00:00, run_id=manual__2025-02-05T06:41:54+00:00, run_start_date=2025-02-05 06:41:55.194513+00:00, run_end_date=2025-02-05 06:47:08.461564+00:00, run_duration=313.267051, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 12:19:05,975 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>
2025-02-05 12:19:05,975 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:19:05,976 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>
2025-02-05 12:19:05,977 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:19:05,978 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:49:04+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:19:05,978 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:49:04+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:19:05,985 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:49:04+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:19:09,104 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:49:04+00:00', try_number=1, map_index=-1)
2025-02-05 12:19:09,108 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:49:04+00:00, map_index=-1, run_start_date=2025-02-05 06:49:07.757478+00:00, run_end_date=2025-02-05 06:49:08.593427+00:00, run_duration=0.835949, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:49:05.977131+00:00, queued_by_job_id=12, pid=279289
2025-02-05 12:19:46,029 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:24:09,422 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>
2025-02-05 12:24:09,422 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:24:09,423 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>
2025-02-05 12:24:09,425 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:24:09,425 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:49:04+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:24:09,426 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:49:04+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:24:09,432 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:49:04+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:24:12,768 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:49:04+00:00', try_number=2, map_index=-1)
2025-02-05 12:24:12,772 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:49:04+00:00, map_index=-1, run_start_date=2025-02-05 06:54:11.280949+00:00, run_end_date=2025-02-05 06:54:12.241312+00:00, run_duration=0.960363, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:54:09.424190+00:00, queued_by_job_id=12, pid=280703
2025-02-05 12:24:46,223 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:29:12,449 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>
2025-02-05 12:29:12,450 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:29:12,450 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>
2025-02-05 12:29:12,452 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T06:49:04+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:29:12,452 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:49:04+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:29:12,452 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:49:04+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:29:12,459 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T06:49:04+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:29:22,641 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T06:49:04+00:00', try_number=3, map_index=-1)
2025-02-05 12:29:22,646 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T06:49:04+00:00, map_index=-1, run_start_date=2025-02-05 06:59:14.596431+00:00, run_end_date=2025-02-05 06:59:22.053551+00:00, run_duration=7.45712, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 06:59:12.451331+00:00, queued_by_job_id=12, pid=281631
2025-02-05 12:29:22,944 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 06:49:04+00:00: manual__2025-02-05T06:49:04+00:00, state:running, queued_at: 2025-02-05 06:49:04.782094+00:00. externally triggered: True> failed
2025-02-05 12:29:22,944 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 06:49:04+00:00, run_id=manual__2025-02-05T06:49:04+00:00, run_start_date=2025-02-05 06:49:05.948918+00:00, run_end_date=2025-02-05 06:59:22.944650+00:00, run_duration=616.995732, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=1bfe0ee7f7d46e1f6376c74d0bf8b94d
2025-02-05 12:29:46,393 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:34:46,559 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:39:46,717 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:40:42,525 INFO - Loaded executor: SequentialExecutor
2025-02-05 12:40:42,561 INFO - Starting the scheduler
2025-02-05 12:40:42,562 INFO - Processing each file at most -1 times
2025-02-05 12:40:42,568 INFO - Launched DagFileProcessorManager with pid: 283288
2025-02-05 12:40:42,570 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:40:42,572 INFO - Configured default timezone UTC
2025-02-05 12:41:20,688 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
2025-02-05 12:41:20,689 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:41:20,689 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
2025-02-05 12:41:20,691 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:41:20,692 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:41:20,692 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:41:20,699 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:41:30,095 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=1, map_index=-1)
2025-02-05 12:41:30,102 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:11:19.267322+00:00, map_index=-1, run_start_date=2025-02-05 07:11:22.712656+00:00, run_end_date=2025-02-05 07:11:29.556950+00:00, run_duration=6.844294, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:11:20.690488+00:00, queued_by_job_id=46, pid=283557
2025-02-05 12:45:42,762 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:45:42,764 INFO - Marked 1 SchedulerJob instances as failed
2025-02-05 12:46:29,944 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
2025-02-05 12:46:29,945 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:46:29,945 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
2025-02-05 12:46:29,948 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:46:29,949 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:46:29,950 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:46:29,956 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:46:33,624 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=2, map_index=-1)
2025-02-05 12:46:33,628 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:11:19.267322+00:00, map_index=-1, run_start_date=2025-02-05 07:16:32.223417+00:00, run_end_date=2025-02-05 07:16:33.085220+00:00, run_duration=0.861803, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:16:29.947006+00:00, queued_by_job_id=46, pid=285774
2025-02-05 12:46:33,835 ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 07:11:19.267322+00:00: manual__2025-02-05T07:11:19.267322+00:00, state:running, queued_at: 2025-02-05 07:11:19.280115+00:00. externally triggered: True> failed
2025-02-05 12:46:33,836 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 07:11:19.267322+00:00, run_id=manual__2025-02-05T07:11:19.267322+00:00, run_start_date=2025-02-05 07:11:20.652095+00:00, run_end_date=2025-02-05 07:16:33.836158+00:00, run_duration=313.184063, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 12:50:42,911 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:52:59,896 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
2025-02-05 12:52:59,896 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:52:59,897 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
2025-02-05 12:52:59,898 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:52:59,899 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:52:59,899 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:52:59,906 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:53:22,864 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=1, map_index=-1)
2025-02-05 12:53:22,868 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:22:58.421946+00:00, map_index=-1, run_start_date=2025-02-05 07:23:02.079515+00:00, run_end_date=2025-02-05 07:23:22.256967+00:00, run_duration=20.177452, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:22:59.897843+00:00, queued_by_job_id=46, pid=286850
2025-02-05 12:55:43,205 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 12:58:22,631 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
2025-02-05 12:58:22,631 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 12:58:22,631 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
2025-02-05 12:58:22,633 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 12:58:22,633 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 12:58:22,634 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:58:22,640 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 12:58:36,285 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=2, map_index=-1)
2025-02-05 12:58:36,289 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:22:58.421946+00:00, map_index=-1, run_start_date=2025-02-05 07:28:24.759646+00:00, run_end_date=2025-02-05 07:28:35.709375+00:00, run_duration=10.949729, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:28:22.632493+00:00, queued_by_job_id=46, pid=289022
2025-02-05 12:58:36,583 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 07:22:58.421946+00:00: manual__2025-02-05T07:22:58.421946+00:00, state:running, queued_at: 2025-02-05 07:22:58.448980+00:00. externally triggered: True> successful
2025-02-05 12:58:36,584 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 07:22:58.421946+00:00, run_id=manual__2025-02-05T07:22:58.421946+00:00, run_start_date=2025-02-05 07:22:59.844597+00:00, run_end_date=2025-02-05 07:28:36.583946+00:00, run_duration=336.739349, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 13:00:43,371 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:05:43,517 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:10:43,591 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:15:43,733 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:20:43,876 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:25:44,177 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:27:57,607 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [scheduled]>
2025-02-05 13:27:57,608 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 13:27:57,608 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [scheduled]>
2025-02-05 13:27:57,609 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 13:27:57,610 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:57:56.302963+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 13:27:57,610 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:57:56.302963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 13:27:57,616 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:57:56.302963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 13:28:01,235 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:57:56.302963+00:00', try_number=1, map_index=-1)
2025-02-05 13:28:01,239 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:57:56.302963+00:00, map_index=-1, run_start_date=2025-02-05 07:57:59.612637+00:00, run_end_date=2025-02-05 07:58:00.689153+00:00, run_duration=1.076516, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:57:57.608887+00:00, queued_by_job_id=46, pid=291318
2025-02-05 13:28:01,494 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 07:57:56.302963+00:00: manual__2025-02-05T07:57:56.302963+00:00, state:running, queued_at: 2025-02-05 07:57:56.331138+00:00. externally triggered: True> successful
2025-02-05 13:28:01,495 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 07:57:56.302963+00:00, run_id=manual__2025-02-05T07:57:56.302963+00:00, run_start_date=2025-02-05 07:57:57.583570+00:00, run_end_date=2025-02-05 07:58:01.495144+00:00, run_duration=3.911574, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 13:30:44,329 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:35:44,446 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:40:44,594 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:45:44,743 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:50:44,922 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 13:55:45,039 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:00:45,185 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:05:45,243 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:10:24,241 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [scheduled]>
2025-02-05 14:10:24,242 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 14:10:24,242 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [scheduled]>
2025-02-05 14:10:24,244 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 14:10:24,244 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:40:23.623285+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 14:10:24,245 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:40:23.623285+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:10:24,251 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:40:23.623285+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:10:33,841 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:40:23.623285+00:00', try_number=1, map_index=-1)
2025-02-05 14:10:33,845 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:40:23.623285+00:00, map_index=-1, run_start_date=2025-02-05 08:40:26.389205+00:00, run_end_date=2025-02-05 08:40:33.281916+00:00, run_duration=6.892711, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:40:24.243297+00:00, queued_by_job_id=46, pid=294856
2025-02-05 14:10:34,033 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:40:23.623285+00:00: manual__2025-02-05T08:40:23.623285+00:00, state:running, queued_at: 2025-02-05 08:40:23.634217+00:00. externally triggered: True> successful
2025-02-05 14:10:34,034 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:40:23.623285+00:00, run_id=manual__2025-02-05T08:40:23.623285+00:00, run_start_date=2025-02-05 08:40:24.213473+00:00, run_end_date=2025-02-05 08:40:34.033995+00:00, run_duration=9.820522, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 14:10:45,381 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:15:45,619 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:20:45,767 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:22:56,209 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [scheduled]>
2025-02-05 14:22:56,209 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 14:22:56,209 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [scheduled]>
2025-02-05 14:22:56,211 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 14:22:56,212 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:52:55+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 14:22:56,212 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:52:55+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:22:56,219 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:52:55+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:22:59,290 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:52:55+00:00', try_number=1, map_index=-1)
2025-02-05 14:22:59,293 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:52:55+00:00, map_index=-1, run_start_date=2025-02-05 08:52:58.031574+00:00, run_end_date=2025-02-05 08:52:58.799696+00:00, run_duration=0.768122, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:52:56.210597+00:00, queued_by_job_id=46, pid=296434
2025-02-05 14:22:59,570 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:52:55+00:00: manual__2025-02-05T08:52:55+00:00, state:running, queued_at: 2025-02-05 08:52:55.945062+00:00. externally triggered: True> successful
2025-02-05 14:22:59,570 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:52:55+00:00, run_id=manual__2025-02-05T08:52:55+00:00, run_start_date=2025-02-05 08:52:56.177994+00:00, run_end_date=2025-02-05 08:52:59.570554+00:00, run_duration=3.39256, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 14:25:46,000 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:27:20,434 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [scheduled]>
2025-02-05 14:27:20,434 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 14:27:20,435 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [scheduled]>
2025-02-05 14:27:20,436 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 14:27:20,436 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:57:19.525810+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 14:27:20,437 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:57:19.525810+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:27:20,443 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:57:19.525810+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:27:33,993 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:57:19.525810+00:00', try_number=1, map_index=-1)
2025-02-05 14:27:33,998 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:57:19.525810+00:00, map_index=-1, run_start_date=2025-02-05 08:57:22.419763+00:00, run_end_date=2025-02-05 08:57:33.452857+00:00, run_duration=11.033094, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:57:20.435800+00:00, queued_by_job_id=46, pid=297042
2025-02-05 14:27:34,188 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:57:19.525810+00:00: manual__2025-02-05T08:57:19.525810+00:00, state:running, queued_at: 2025-02-05 08:57:19.539313+00:00. externally triggered: True> successful
2025-02-05 14:27:34,188 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:57:19.525810+00:00, run_id=manual__2025-02-05T08:57:19.525810+00:00, run_start_date=2025-02-05 08:57:20.411069+00:00, run_end_date=2025-02-05 08:57:34.188864+00:00, run_duration=13.777795, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 14:28:10,318 INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [scheduled]>
2025-02-05 14:28:10,319 INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 14:28:10,319 INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [scheduled]>
2025-02-05 14:28:10,320 INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 14:28:10,321 INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:58:09.766391+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 14:28:10,321 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:58:09.766391+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:28:10,327 INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:58:09.766391+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 14:28:13,781 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:58:09.766391+00:00', try_number=1, map_index=-1)
2025-02-05 14:28:13,785 INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:58:09.766391+00:00, map_index=-1, run_start_date=2025-02-05 08:58:12.325770+00:00, run_end_date=2025-02-05 08:58:13.240928+00:00, run_duration=0.915158, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:58:10.320032+00:00, queued_by_job_id=46, pid=297243
2025-02-05 14:28:13,989 INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:58:09.766391+00:00: manual__2025-02-05T08:58:09.766391+00:00, state:running, queued_at: 2025-02-05 08:58:09.774758+00:00. externally triggered: True> successful
2025-02-05 14:28:13,989 INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:58:09.766391+00:00, run_id=manual__2025-02-05T08:58:09.766391+00:00, run_start_date=2025-02-05 08:58:10.293335+00:00, run_end_date=2025-02-05 08:58:13.989542+00:00, run_duration=3.696207, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
2025-02-05 14:30:46,166 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:35:46,407 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:40:46,564 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:45:46,865 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:50:47,150 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 14:55:47,320 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:00:50,629 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:05:50,883 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:10:51,047 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:15:51,345 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:20:52,128 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:25:52,172 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:30:52,373 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:35:52,519 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:40:52,708 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:45:52,886 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:50:53,740 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 15:55:53,887 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:00:54,049 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:05:54,802 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:10:40,680 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-05 00:00:00+00:00, run_after=2025-02-06 00:00:00+00:00
2025-02-05 16:10:40,706 INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 00:00:00+00:00: scheduled__2025-02-04T00:00:00+00:00, state:running, queued_at: 2025-02-05 10:40:40.671494+00:00. externally triggered: False> successful
2025-02-05 16:10:40,707 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 00:00:00+00:00, run_id=scheduled__2025-02-04T00:00:00+00:00, run_start_date=2025-02-05 10:40:40.694390+00:00, run_end_date=2025-02-05 10:40:40.707042+00:00, run_duration=0.012652, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
2025-02-05 16:10:40,710 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-05 00:00:00+00:00, run_after=2025-02-06 00:00:00+00:00
2025-02-05 16:10:40,713 ERROR - Failed to get task for ti <TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [up_for_retry]>. Marking it as removed.
2025-02-05 16:10:40,716 INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 07:13:04.631466+00:00: manual__2025-02-04T07:13:04.631466+00:00, state:running, queued_at: 2025-02-04 07:13:04.653327+00:00. externally triggered: True> successful
2025-02-05 16:10:40,716 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 07:13:04.631466+00:00, run_id=manual__2025-02-04T07:13:04.631466+00:00, run_start_date=2025-02-04 07:13:06.122188+00:00, run_end_date=2025-02-05 10:40:40.716437+00:00, run_duration=98854.594249, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-03 07:13:04.631466+00:00, data_interval_end=2025-02-04 07:13:04.631466+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
2025-02-05 16:10:54,970 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:15:55,161 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:20:55,327 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:25:55,370 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:30:55,520 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:35:55,680 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:40:55,968 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:45:56,117 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:50:56,401 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 16:55:56,569 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:00:11,381 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
2025-02-05 17:00:11,382 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 17:00:11,382 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
2025-02-05 17:00:11,383 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 17:00:11,384 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 17:00:11,384 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 17:00:11,390 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 17:00:14,709 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=1, map_index=-1)
2025-02-05 17:00:14,713 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_and_store_brewery_data, run_id=manual__2025-02-05T11:30:10.100554+00:00, map_index=-1, run_start_date=2025-02-05 11:30:13.359348+00:00, run_end_date=2025-02-05 11:30:14.147943+00:00, run_duration=0.788595, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 11:30:11.382824+00:00, queued_by_job_id=46, pid=313612
2025-02-05 17:00:56,727 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:05:14,673 INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
2025-02-05 17:05:14,673 INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
2025-02-05 17:05:14,673 INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
2025-02-05 17:05:14,675 INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-02-05 17:05:14,675 INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2025-02-05 17:05:14,675 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 17:05:14,681 INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
2025-02-05 17:05:17,871 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=2, map_index=-1)
2025-02-05 17:05:17,875 INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_and_store_brewery_data, run_id=manual__2025-02-05T11:30:10.100554+00:00, map_index=-1, run_start_date=2025-02-05 11:35:16.518485+00:00, run_end_date=2025-02-05 11:35:17.369896+00:00, run_duration=0.851411, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 11:35:14.674354+00:00, queued_by_job_id=46, pid=315825
2025-02-05 17:05:18,080 ERROR - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-05 11:30:10.100554+00:00: manual__2025-02-05T11:30:10.100554+00:00, state:running, queued_at: 2025-02-05 11:30:10.113844+00:00. externally triggered: True> failed
2025-02-05 17:05:18,081 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-05 11:30:10.100554+00:00, run_id=manual__2025-02-05T11:30:10.100554+00:00, run_start_date=2025-02-05 11:30:11.352145+00:00, run_end_date=2025-02-05 11:35:18.081246+00:00, run_duration=306.729101, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=684996f830e62e0e542494cfaa1b66e4
2025-02-05 17:05:56,834 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:08:56,426 INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-05 11:38:55.209497+00:00: manual__2025-02-05T11:38:55.209497+00:00, state:running, queued_at: 2025-02-05 11:38:55.222285+00:00. externally triggered: True> successful
2025-02-05 17:08:56,427 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-05 11:38:55.209497+00:00, run_id=manual__2025-02-05T11:38:55.209497+00:00, run_start_date=2025-02-05 11:38:56.414475+00:00, run_end_date=2025-02-05 11:38:56.427353+00:00, run_duration=0.012878, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
2025-02-05 17:10:57,031 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:15:58,031 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:20:58,891 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:25:59,176 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:30:59,304 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:35:59,974 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:41:00,148 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:46:00,296 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:51:14,137 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 17:56:14,312 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 18:01:14,518 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 18:06:20,875 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 18:11:21,056 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 18:16:21,235 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-05 18:21:21,307 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:25:44,439 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-06 00:00:00+00:00, run_after=2025-02-07 00:00:00+00:00
2025-02-06 10:25:44,478 INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-05 00:00:00+00:00: scheduled__2025-02-05T00:00:00+00:00, state:running, queued_at: 2025-02-06 04:55:44.433870+00:00. externally triggered: False> successful
2025-02-06 10:25:44,478 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-05 00:00:00+00:00, run_id=scheduled__2025-02-05T00:00:00+00:00, run_start_date=2025-02-06 04:55:44.456327+00:00, run_end_date=2025-02-06 04:55:44.478694+00:00, run_duration=0.022367, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-05 00:00:00+00:00, data_interval_end=2025-02-06 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
2025-02-06 10:25:44,484 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-06 00:00:00+00:00, run_after=2025-02-07 00:00:00+00:00
2025-02-06 10:25:44,532 INFO - Heartbeat recovered after 57621.56 seconds
2025-02-06 10:26:39,570 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:31:39,741 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:36:39,902 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:41:40,057 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:46:40,204 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:51:40,351 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 10:56:40,419 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:01:40,502 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:06:40,647 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:11:40,945 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:16:41,196 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:21:41,364 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:26:41,659 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:31:41,812 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:36:42,005 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:41:45,874 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:46:46,020 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:51:46,321 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 11:56:47,939 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:01:50,527 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:06:50,675 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:11:50,834 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:16:50,982 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:21:51,176 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:26:54,877 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:31:55,175 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:36:55,461 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:41:56,046 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:46:56,857 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:51:57,127 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 12:56:57,301 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:01:57,325 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:06:58,901 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:11:59,048 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:17:02,880 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:22:03,085 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:27:03,252 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:32:05,339 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:37:05,805 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:42:06,804 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:47:06,952 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:52:06,987 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 13:57:06,328 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:02:07,219 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:07:07,181 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:12:12,192 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:17:12,361 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:22:12,505 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:27:12,652 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:32:12,800 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:37:12,967 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:42:13,342 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:47:13,583 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:52:13,748 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 14:57:13,891 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:02:14,057 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:07:14,068 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:12:14,721 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:17:14,866 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:22:15,032 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:27:15,140 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:32:15,320 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:37:15,467 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:42:15,664 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:47:15,832 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:52:16,187 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 15:57:16,357 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:02:16,526 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:07:16,694 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:12:15,345 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:17:20,195 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:22:20,827 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:27:21,133 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:32:25,102 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:37:25,263 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:42:25,558 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:47:26,383 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:52:29,968 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 16:57:28,992 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:02:29,968 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:07:30,331 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:12:30,483 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:17:30,657 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:22:30,825 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:27:31,049 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:32:35,897 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:37:36,062 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:42:40,316 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:47:40,481 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:52:40,628 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 17:57:40,793 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 18:02:40,959 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 18:07:55,346 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 18:12:55,623 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-06 18:13:45,935 INFO - Heartbeat recovered after 33.21 seconds
2025-02-06 18:17:55,792 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-02-07 10:35:36,897 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-07 00:00:00+00:00, run_after=2025-02-08 00:00:00+00:00
2025-02-07 10:35:36,920 INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-06 00:00:00+00:00: scheduled__2025-02-06T00:00:00+00:00, state:running, queued_at: 2025-02-07 05:05:36.894472+00:00. externally triggered: False> successful
2025-02-07 10:35:36,921 INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-06 00:00:00+00:00, run_id=scheduled__2025-02-06T00:00:00+00:00, run_start_date=2025-02-07 05:05:36.908505+00:00, run_end_date=2025-02-07 05:05:36.921136+00:00, run_duration=0.012631, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-06 00:00:00+00:00, data_interval_end=2025-02-07 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
2025-02-07 10:35:36,923 INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-07 00:00:00+00:00, run_after=2025-02-08 00:00:00+00:00
2025-02-07 10:35:36,952 INFO - Heartbeat recovered after 58455.12 seconds
2025-02-07 10:35:44,284 INFO - Exiting gracefully upon receiving signal 15
2025-02-07 10:35:44,442 INFO - Sending 15 to group 283288. PIDs of all processes in the group: []
2025-02-07 10:35:44,443 INFO - Sending the signal 15 to group 283288
2025-02-07 10:35:44,443 INFO - Sending the signal 15 to process 283288 as process group is missing.
2025-02-07 10:35:44,444 INFO - Sending 15 to group 283288. PIDs of all processes in the group: []
2025-02-07 10:35:44,444 INFO - Sending the signal 15 to group 283288
2025-02-07 10:35:44,445 INFO - Sending the signal 15 to process 283288 as process group is missing.
2025-02-07 10:35:44,445 INFO - Exited execute loop
