[2025-01-31T10:38:47.314+0000] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2025-01-31T10:38:47.596+0000] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-01-31T10:38:47.599+0000] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-01-31T10:38:47.614+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 626
[2025-01-31T10:38:47.622+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T10:38:47.641+0000] {scheduler_job_runner.py:1972} INFO - Marked 1 SchedulerJob instances as failed
[2025-01-31T10:38:47.647+0000] {settings.py:63} INFO - Configured default timezone UTC
[2025-01-31T10:38:47.906+0000] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-01-31T10:40:21.090+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 10:40:19.532739+00:00: manual__2025-01-31T10:40:19.532739+00:00, state:running, queued_at: 2025-01-31 10:40:19.558959+00:00. externally triggered: True> successful
[2025-01-31T10:40:21.093+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 10:40:19.532739+00:00, run_id=manual__2025-01-31T10:40:19.532739+00:00, run_start_date=2025-01-31 10:40:21.055124+00:00, run_end_date=2025-01-31 10:40:21.093920+00:00, run_duration=0.038796, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 10:40:19.532739+00:00, data_interval_end=2025-01-31 10:40:19.532739+00:00, dag_hash=aeec7285723403828e458d11669d5445
[2025-01-31T10:41:28.733+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 10:41:24.404228+00:00: manual__2025-01-31T10:41:24.404228+00:00, state:running, queued_at: 2025-01-31 10:41:24.434007+00:00. externally triggered: True> successful
[2025-01-31T10:41:28.736+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 10:41:24.404228+00:00, run_id=manual__2025-01-31T10:41:24.404228+00:00, run_start_date=2025-01-31 10:41:28.706728+00:00, run_end_date=2025-01-31 10:41:28.736462+00:00, run_duration=0.029734, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 10:41:24.404228+00:00, data_interval_end=2025-01-31 10:41:24.404228+00:00, dag_hash=aeec7285723403828e458d11669d5445
[2025-01-31T10:42:14.439+0000] {job.py:229} INFO - Heartbeat recovered after 35.30 seconds
[2025-01-31T10:43:47.638+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T10:47:14.874+0000] {job.py:229} INFO - Heartbeat recovered after 37.20 seconds
[2025-01-31T10:48:44.962+0000] {job.py:229} INFO - Heartbeat recovered after 34.14 seconds
[2025-01-31T10:48:15.123+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 10:48:14.436080+00:00: manual__2025-01-31T10:48:14.436080+00:00, state:running, queued_at: 2025-01-31 10:48:14.468342+00:00. externally triggered: True> successful
[2025-01-31T10:48:15.127+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 10:48:14.436080+00:00, run_id=manual__2025-01-31T10:48:14.436080+00:00, run_start_date=2025-01-31 10:48:15.095744+00:00, run_end_date=2025-01-31 10:48:15.126976+00:00, run_duration=0.031232, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 10:48:14.436080+00:00, data_interval_end=2025-01-31 10:48:14.436080+00:00, dag_hash=aeec7285723403828e458d11669d5445
[2025-01-31T10:48:48.631+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T10:49:55.143+0000] {job.py:229} INFO - Heartbeat recovered after 43.57 seconds
[2025-01-31T10:51:35.197+0000] {job.py:229} INFO - Heartbeat recovered after 38.11 seconds
[2025-01-31T10:53:15.384+0000] {job.py:229} INFO - Heartbeat recovered after 34.21 seconds
[2025-01-31T10:54:20.491+0000] {job.py:229} INFO - Heartbeat recovered after 38.20 seconds
[2025-01-31T10:53:48.557+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T10:58:50.931+0000] {job.py:229} INFO - Heartbeat recovered after 36.27 seconds
[2025-01-31T10:58:48.260+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T10:59:46.057+0000] {job.py:229} INFO - Heartbeat recovered after 37.30 seconds
[2025-01-31T11:01:06.132+0000] {job.py:229} INFO - Heartbeat recovered after 38.27 seconds
[2025-01-31T11:02:01.307+0000] {job.py:229} INFO - Heartbeat recovered after 35.26 seconds
[2025-01-31T11:03:31.430+0000] {job.py:229} INFO - Heartbeat recovered after 38.25 seconds
[2025-01-31T11:03:52.438+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:05:51.634+0000] {job.py:229} INFO - Heartbeat recovered after 38.12 seconds
[2025-01-31T11:07:21.954+0000] {job.py:229} INFO - Heartbeat recovered after 37.96 seconds
[2025-01-31T11:08:52.388+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:13:46.054+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 11:13:44.821287+00:00: manual__2025-01-31T11:13:44.821287+00:00, state:running, queued_at: 2025-01-31 11:13:44.856684+00:00. externally triggered: True> successful
[2025-01-31T11:13:46.058+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 11:13:44.821287+00:00, run_id=manual__2025-01-31T11:13:44.821287+00:00, run_start_date=2025-01-31 11:13:46.028132+00:00, run_end_date=2025-01-31 11:13:46.058076+00:00, run_duration=0.029944, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 11:13:44.821287+00:00, data_interval_end=2025-01-31 11:13:44.821287+00:00, dag_hash=aeec7285723403828e458d11669d5445
[2025-01-31T11:13:52.362+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:17:07.785+0000] {job.py:229} INFO - Heartbeat recovered after 38.14 seconds
[2025-01-31T11:16:48.232+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 11:16:43.293143+00:00: manual__2025-01-31T11:16:43.293143+00:00, state:running, queued_at: 2025-01-31 11:16:43.323723+00:00. externally triggered: True> successful
[2025-01-31T11:16:48.235+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 11:16:43.293143+00:00, run_id=manual__2025-01-31T11:16:43.293143+00:00, run_start_date=2025-01-31 11:16:48.189770+00:00, run_end_date=2025-01-31 11:16:48.235665+00:00, run_duration=0.045895, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 11:16:43.293143+00:00, data_interval_end=2025-01-31 11:16:43.293143+00:00, dag_hash=aeec7285723403828e458d11669d5445
[2025-01-31T11:18:51.645+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:21:29.252+0000] {job.py:229} INFO - Heartbeat recovered after 62.81 seconds
[2025-01-31T11:21:23.966+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 11:21:22.876214+00:00: manual__2025-01-31T11:21:22.876214+00:00, state:running, queued_at: 2025-01-31 11:21:22.923452+00:00. externally triggered: True> successful
[2025-01-31T11:21:23.973+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 11:21:22.876214+00:00, run_id=manual__2025-01-31T11:21:22.876214+00:00, run_start_date=2025-01-31 11:21:23.809670+00:00, run_end_date=2025-01-31 11:21:23.973739+00:00, run_duration=0.164069, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 11:21:22.876214+00:00, data_interval_end=2025-01-31 11:21:22.876214+00:00, dag_hash=aeec7285723403828e458d11669d5445
[2025-01-31T11:22:45.292+0000] {job.py:229} INFO - Heartbeat recovered after 35.94 seconds
[2025-01-31T11:24:24.646+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:25:05.269+0000] {job.py:229} INFO - Heartbeat recovered after 31.71 seconds
[2025-01-31T11:26:18.791+0000] {job.py:229} INFO - Heartbeat recovered after 32.91 seconds
[2025-01-31T11:27:18.809+0000] {job.py:229} INFO - Heartbeat recovered after 38.30 seconds
[2025-01-31T11:29:13.952+0000] {job.py:229} INFO - Heartbeat recovered after 36.33 seconds
[2025-01-31T11:28:55.230+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:30:54.117+0000] {job.py:229} INFO - Heartbeat recovered after 34.52 seconds
[2025-01-31T11:33:24.334+0000] {job.py:229} INFO - Heartbeat recovered after 36.26 seconds
[2025-01-31T11:33:54.563+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:34:34.434+0000] {job.py:229} INFO - Heartbeat recovered after 37.29 seconds
[2025-01-31T11:35:24.237+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-01-31T11:35:17.158630+00:00 [scheduled]>
[2025-01-31T11:35:24.240+0000] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-01-31T11:35:24.243+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-01-31T11:35:17.158630+00:00 [scheduled]>
[2025-01-31T11:35:24.248+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-01-31T11:35:17.158630+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-31T11:35:24.250+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-01-31T11:35:17.158630+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-01-31T11:35:24.253+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-01-31T11:35:17.158630+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-01-31T11:35:24.275+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-01-31T11:35:17.158630+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-01-31T11:35:43.069+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/pipeline_miniproject.py
[2025-01-31T11:35:50.206+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-01-31T11:35:17.158630+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-01-31T11:35:53.057+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-01-31T11:35:17.158630+00:00', try_number=1, map_index=-1)
[2025-01-31T11:35:53.066+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-01-31T11:35:17.158630+00:00, map_index=-1, run_start_date=2025-01-31 11:35:50.421824+00:00, run_end_date=2025-01-31 11:35:52.450948+00:00, run_duration=2.029124, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-01-31 11:35:24.246134+00:00, queued_by_job_id=27, pid=3351
[2025-01-31T11:35:53.105+0000] {job.py:229} INFO - Heartbeat recovered after 39.73 seconds
[2025-01-31T11:35:53.567+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-01-31 11:35:17.158630+00:00: manual__2025-01-31T11:35:17.158630+00:00, state:running, queued_at: 2025-01-31 11:35:17.196709+00:00. externally triggered: True> successful
[2025-01-31T11:35:53.570+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-01-31 11:35:17.158630+00:00, run_id=manual__2025-01-31T11:35:17.158630+00:00, run_start_date=2025-01-31 11:35:24.181915+00:00, run_end_date=2025-01-31 11:35:53.570778+00:00, run_duration=29.388863, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-30 11:35:17.158630+00:00, data_interval_end=2025-01-31 11:35:17.158630+00:00, dag_hash=ce2304765204139f7aeda71fd2091116
[2025-01-31T11:37:44.677+0000] {job.py:229} INFO - Heartbeat recovered after 30.04 seconds
[2025-01-31T11:38:54.514+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:39:44.855+0000] {job.py:229} INFO - Heartbeat recovered after 37.28 seconds
[2025-01-31T11:43:54.474+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:45:05.545+0000] {job.py:229} INFO - Heartbeat recovered after 60.45 seconds
[2025-01-31T11:48:00.733+0000] {job.py:229} INFO - Heartbeat recovered after 38.26 seconds
[2025-01-31T11:48:56.108+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:49:40.942+0000] {job.py:229} INFO - Heartbeat recovered after 34.38 seconds
[2025-01-31T11:52:11.214+0000] {job.py:229} INFO - Heartbeat recovered after 38.42 seconds
[2025-01-31T11:52:51.244+0000] {job.py:229} INFO - Heartbeat recovered after 35.38 seconds
[2025-01-31T11:53:57.458+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:57:06.636+0000] {job.py:229} INFO - Heartbeat recovered after 37.83 seconds
[2025-01-31T11:57:46.757+0000] {job.py:229} INFO - Heartbeat recovered after 34.16 seconds
[2025-01-31T11:58:57.375+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T11:59:54.875+0000] {job.py:229} INFO - Heartbeat recovered after 44.48 seconds
[2025-01-31T12:01:12.073+0000] {job.py:229} INFO - Heartbeat recovered after 38.64 seconds
[2025-01-31T12:02:17.197+0000] {job.py:229} INFO - Heartbeat recovered after 34.62 seconds
[2025-01-31T12:02:57.148+0000] {job.py:229} INFO - Heartbeat recovered after 34.95 seconds
[2025-01-31T12:04:12.963+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:09:21.064+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:12:31.869+0000] {job.py:229} INFO - Heartbeat recovered after 64.26 seconds
[2025-01-31T12:14:21.701+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:19:21.651+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:19:58.561+0000] {job.py:229} INFO - Heartbeat recovered after 35.78 seconds
[2025-01-31T12:20:28.615+0000] {job.py:229} INFO - Heartbeat recovered after 30.07 seconds
[2025-01-31T12:23:28.972+0000] {job.py:229} INFO - Heartbeat recovered after 34.38 seconds
[2025-01-31T12:24:21.538+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:28:14.275+0000] {job.py:229} INFO - Heartbeat recovered after 35.07 seconds
[2025-01-31T12:29:54.378+0000] {job.py:229} INFO - Heartbeat recovered after 41.36 seconds
[2025-01-31T12:29:54.388+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:29:54.395+0000] {scheduler_job_runner.py:1972} INFO - Marked 1 SchedulerJob instances as failed
[2025-01-31T12:30:24.512+0000] {job.py:229} INFO - Heartbeat recovered after 36.33 seconds
[2025-01-31T12:31:48.644+0000] {job.py:229} INFO - Heartbeat recovered after 42.51 seconds
[2025-01-31T12:34:22.415+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:36:10.036+0000] {job.py:229} INFO - Heartbeat recovered after 38.21 seconds
[2025-01-31T12:37:20.111+0000] {job.py:229} INFO - Heartbeat recovered after 44.24 seconds
[2025-01-31T12:38:15.179+0000] {job.py:229} INFO - Heartbeat recovered after 38.99 seconds
[2025-01-31T12:39:23.413+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:44:24.345+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:49:16.096+0000] {job.py:229} INFO - Heartbeat recovered after 37.34 seconds
[2025-01-31T12:49:23.725+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-31T12:50:06.182+0000] {job.py:229} INFO - Heartbeat recovered after 34.17 seconds
[2025-02-03T05:17:51.558+0000] {dag.py:4180} INFO - Setting next_dagrun for hey_ud_dag to 2025-02-01 23:00:00+00:00, run_after=2025-02-02 23:00:00+00:00
[2025-02-03T05:17:51.763+0000] {dag.py:4180} INFO - Setting next_dagrun for weather_etl_pipeline to 2025-02-02 00:00:00+00:00, run_after=2025-02-03 00:00:00+00:00
[2025-02-03T05:17:52.929+0000] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-01-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: hey_ud_dag.print_welcome scheduled__2025-01-30T23:00:00+00:00 [scheduled]>
[2025-02-03T05:17:52.948+0000] {scheduler_job_runner.py:507} INFO - DAG weather_etl_pipeline has 0/16 running and queued tasks
[2025-02-03T05:17:52.951+0000] {scheduler_job_runner.py:507} INFO - DAG hey_ud_dag has 0/16 running and queued tasks
[2025-02-03T05:17:52.955+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-01-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: hey_ud_dag.print_welcome scheduled__2025-01-30T23:00:00+00:00 [scheduled]>
[2025-02-03T05:17:52.960+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-01-31T00:00:00+00:00 [scheduled]>, <TaskInstance: hey_ud_dag.print_welcome scheduled__2025-01-30T23:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-03T05:17:52.966+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='weather_etl_pipeline', task_id='extract_weather_data', run_id='scheduled__2025-01-31T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-03T05:17:52.973+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'weather_etl_pipeline', 'extract_weather_data', 'scheduled__2025-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_weather.py']
[2025-02-03T05:17:52.977+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_welcome', run_id='scheduled__2025-01-30T23:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-03T05:17:52.979+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_welcome', 'scheduled__2025-01-30T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:17:53.223+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'weather_etl_pipeline', 'extract_weather_data', 'scheduled__2025-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_weather.py']
[2025-02-03T05:19:00.759+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/etl_weather.py
[2025-02-03T05:19:04.261+0000] {task_command.py:467} INFO - Running <TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-01-31T00:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:19:17.472+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_welcome', 'scheduled__2025-01-30T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:20:11.707+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/exampledag.py
[2025-02-03T05:19:35.842+0000] {task_command.py:467} INFO - Running <TaskInstance: hey_ud_dag.print_welcome scheduled__2025-01-30T23:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:19:37.904+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='weather_etl_pipeline', task_id='extract_weather_data', run_id='scheduled__2025-01-31T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:19:37.906+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_welcome', run_id='scheduled__2025-01-30T23:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:19:37.913+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=hey_ud_dag, task_id=print_welcome, run_id=scheduled__2025-01-30T23:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:19:36.542020+00:00, run_end_date=2025-02-03 05:19:37.436014+00:00, run_duration=0.893994, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=30, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-02-03 05:17:52.958950+00:00, queued_by_job_id=27, pid=7842
[2025-02-03T05:19:37.917+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=weather_etl_pipeline, task_id=extract_weather_data, run_id=scheduled__2025-01-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:19:04.469965+00:00, run_end_date=2025-02-03 05:19:16.580835+00:00, run_duration=12.11087, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=29, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-02-03 05:17:52.958950+00:00, queued_by_job_id=27, pid=7818
[2025-02-03T05:19:37.931+0000] {manager.py:293} ERROR - DagFileProcessorManager (PID=626) last sent a heartbeat 143.14 seconds ago! Restarting it
[2025-02-03T05:19:37.937+0000] {process_utils.py:132} INFO - Sending 15 to group 626. PIDs of all processes in the group: [626]
[2025-02-03T05:19:37.940+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 626
[2025-02-03T05:19:51.029+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=626, status='terminated', exitcode=0, started='03:04:49') (626) terminated with exit code 0
[2025-02-03T05:19:51.033+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 7850
[2025-02-03T05:19:51.062+0000] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-03T05:19:51.074+0000] {job.py:229} INFO - Heartbeat recovered after 232184.92 seconds
[2025-02-03T05:19:51.331+0000] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-03T05:20:02.986+0000] {dag.py:4180} INFO - Setting next_dagrun for weather_etl_pipeline to 2025-02-03 00:00:00+00:00, run_after=2025-02-04 00:00:00+00:00
[2025-02-03T05:20:02.999+0000] {dag.py:4180} INFO - Setting next_dagrun for hey_ud_dag to 2025-02-02 23:00:00+00:00, run_after=2025-02-03 23:00:00+00:00
[2025-02-03T05:20:03.016+0000] {dag.py:4180} INFO - Setting next_dagrun for brewery_data_pipeline to 2025-02-03 00:00:00+00:00, run_after=2025-02-04 00:00:00+00:00
[2025-02-03T05:20:03.156+0000] {scheduler_job_runner.py:435} INFO - 4 tasks up for execution:
	<TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-02-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: hey_ud_dag.print_welcome scheduled__2025-02-01T23:00:00+00:00 [scheduled]>
	<TaskInstance: hey_ud_dag.print_date scheduled__2025-01-30T23:00:00+00:00 [scheduled]>
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-02T00:00:00+00:00 [scheduled]>
[2025-02-03T05:20:03.158+0000] {scheduler_job_runner.py:507} INFO - DAG weather_etl_pipeline has 0/16 running and queued tasks
[2025-02-03T05:20:03.161+0000] {scheduler_job_runner.py:507} INFO - DAG hey_ud_dag has 0/16 running and queued tasks
[2025-02-03T05:20:03.163+0000] {scheduler_job_runner.py:507} INFO - DAG hey_ud_dag has 1/16 running and queued tasks
[2025-02-03T05:20:03.165+0000] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-03T05:20:03.167+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-02-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: hey_ud_dag.print_welcome scheduled__2025-02-01T23:00:00+00:00 [scheduled]>
	<TaskInstance: hey_ud_dag.print_date scheduled__2025-01-30T23:00:00+00:00 [scheduled]>
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-02T00:00:00+00:00 [scheduled]>
[2025-02-03T05:20:03.171+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-02-02T00:00:00+00:00 [scheduled]>, <TaskInstance: hey_ud_dag.print_welcome scheduled__2025-02-01T23:00:00+00:00 [scheduled]>, <TaskInstance: hey_ud_dag.print_date scheduled__2025-01-30T23:00:00+00:00 [scheduled]>, <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-03T05:20:03.173+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='weather_etl_pipeline', task_id='extract_weather_data', run_id='scheduled__2025-02-02T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-03T05:20:03.175+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'weather_etl_pipeline', 'extract_weather_data', 'scheduled__2025-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_weather.py']
[2025-02-03T05:20:03.177+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_welcome', run_id='scheduled__2025-02-01T23:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-03T05:20:03.179+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_welcome', 'scheduled__2025-02-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:20:03.182+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_date', run_id='scheduled__2025-01-30T23:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-03T05:20:03.184+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_date', 'scheduled__2025-01-30T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:20:03.186+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-02T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-03T05:20:03.187+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-03T05:20:03.208+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'weather_etl_pipeline', 'extract_weather_data', 'scheduled__2025-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_weather.py']
[2025-02-03T05:20:35.038+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/etl_weather.py
[2025-02-03T05:20:39.721+0000] {task_command.py:467} INFO - Running <TaskInstance: weather_etl_pipeline.extract_weather_data scheduled__2025-02-02T00:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:20:51.158+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_welcome', 'scheduled__2025-02-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:21:14.380+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/exampledag.py
[2025-02-03T05:21:15.010+0000] {task_command.py:467} INFO - Running <TaskInstance: hey_ud_dag.print_welcome scheduled__2025-02-01T23:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:21:16.166+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_date', 'scheduled__2025-01-30T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:21:36.648+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/exampledag.py
[2025-02-03T05:21:37.231+0000] {task_command.py:467} INFO - Running <TaskInstance: hey_ud_dag.print_date scheduled__2025-01-30T23:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:21:39.702+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'scheduled__2025-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-03T05:22:03.768+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/pipeline_miniproject.py
[2025-02-03T05:22:17.504+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data scheduled__2025-02-02T00:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:22:20.025+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='weather_etl_pipeline', task_id='extract_weather_data', run_id='scheduled__2025-02-02T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:22:20.027+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_welcome', run_id='scheduled__2025-02-01T23:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:22:20.030+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_date', run_id='scheduled__2025-01-30T23:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:22:20.031+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='scheduled__2025-02-02T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:22:20.040+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=hey_ud_dag, task_id=print_date, run_id=scheduled__2025-01-30T23:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:21:37.940411+00:00, run_end_date=2025-02-03 05:21:39.280851+00:00, run_duration=1.34044, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-03 05:20:03.169825+00:00, queued_by_job_id=27, pid=8031
[2025-02-03T05:22:20.043+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=hey_ud_dag, task_id=print_welcome, run_id=scheduled__2025-02-01T23:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:21:51.814665+00:00, run_end_date=2025-02-03 05:21:15.704536+00:00, run_duration=-36.110129, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=32, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-02-03 05:20:03.169825+00:00, queued_by_job_id=27, pid=7907
[2025-02-03T05:22:20.046+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=weather_etl_pipeline, task_id=extract_weather_data, run_id=scheduled__2025-02-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:20:39.972643+00:00, run_end_date=2025-02-03 05:20:50.510981+00:00, run_duration=10.538338, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-02-03 05:20:03.169825+00:00, queued_by_job_id=27, pid=7887
[2025-02-03T05:22:20.049+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=scheduled__2025-02-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:22:17.679327+00:00, run_end_date=2025-02-03 05:22:19.064655+00:00, run_duration=1.385328, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-03 05:20:03.169825+00:00, queued_by_job_id=27, pid=8054
[2025-02-03T05:22:20.063+0000] {manager.py:293} ERROR - DagFileProcessorManager (PID=7850) last sent a heartbeat 137.14 seconds ago! Restarting it
[2025-02-03T05:22:20.067+0000] {process_utils.py:132} INFO - Sending 15 to group 7850. PIDs of all processes in the group: [7850]
[2025-02-03T05:22:20.070+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 7850
[2025-02-03T05:22:32.793+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=7850, status='terminated', exitcode=0, started='05:19:50') (7850) terminated with exit code 0
[2025-02-03T05:22:32.808+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 8062
[2025-02-03T05:22:32.859+0000] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-03T05:22:32.862+0000] {job.py:229} INFO - Heartbeat recovered after 161.81 seconds
[2025-02-03T05:22:32.880+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:22:33.113+0000] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-03T05:23:17.010+0000] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-02 00:00:00+00:00: scheduled__2025-02-02T00:00:00+00:00, state:running, queued_at: 2025-02-03 05:20:03.004863+00:00. externally triggered: False> successful
[2025-02-03T05:23:17.013+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-02 00:00:00+00:00, run_id=scheduled__2025-02-02T00:00:00+00:00, run_start_date=2025-02-03 05:20:03.048707+00:00, run_end_date=2025-02-03 05:23:17.013353+00:00, run_duration=193.964646, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-02 00:00:00+00:00, data_interval_end=2025-02-03 00:00:00+00:00, dag_hash=ce2304765204139f7aeda71fd2091116
[2025-02-03T05:23:17.023+0000] {dag.py:4180} INFO - Setting next_dagrun for brewery_data_pipeline to 2025-02-03 00:00:00+00:00, run_after=2025-02-04 00:00:00+00:00
[2025-02-03T05:23:17.035+0000] {dagrun.py:854} INFO - Marking run <DagRun hey_ud_dag @ 2025-01-30 23:00:00+00:00: scheduled__2025-01-30T23:00:00+00:00, state:running, queued_at: 2025-02-03 05:17:51.509973+00:00. externally triggered: False> successful
[2025-02-03T05:23:17.036+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=hey_ud_dag, execution_date=2025-01-30 23:00:00+00:00, run_id=scheduled__2025-01-30T23:00:00+00:00, run_start_date=2025-02-03 05:17:52.332548+00:00, run_end_date=2025-02-03 05:23:17.036858+00:00, run_duration=324.70431, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-01-30 23:00:00+00:00, data_interval_end=2025-01-31 23:00:00+00:00, dag_hash=ab242d6892e263c657d2549a680f76a0
[2025-02-03T05:23:17.043+0000] {dag.py:4180} INFO - Setting next_dagrun for hey_ud_dag to 2025-02-01 23:00:00+00:00, run_after=2025-02-02 23:00:00+00:00
[2025-02-03T05:23:17.050+0000] {dagrun.py:823} ERROR - Marking run <DagRun weather_etl_pipeline @ 2025-01-31 00:00:00+00:00: scheduled__2025-01-31T00:00:00+00:00, state:running, queued_at: 2025-02-03 05:17:51.757241+00:00. externally triggered: False> failed
[2025-02-03T05:23:17.053+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=weather_etl_pipeline, execution_date=2025-01-31 00:00:00+00:00, run_id=scheduled__2025-01-31T00:00:00+00:00, run_start_date=2025-02-03 05:17:52.332947+00:00, run_end_date=2025-02-03 05:23:17.053342+00:00, run_duration=324.720395, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-01-31 00:00:00+00:00, data_interval_end=2025-02-01 00:00:00+00:00, dag_hash=177d7a241483b43fcf3596b1a4912f6b
[2025-02-03T05:23:17.059+0000] {dag.py:4180} INFO - Setting next_dagrun for weather_etl_pipeline to 2025-02-02 00:00:00+00:00, run_after=2025-02-03 00:00:00+00:00
[2025-02-03T05:22:40.474+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: hey_ud_dag.print_date scheduled__2025-02-01T23:00:00+00:00 [scheduled]>
[2025-02-03T05:22:40.476+0000] {scheduler_job_runner.py:507} INFO - DAG hey_ud_dag has 0/16 running and queued tasks
[2025-02-03T05:22:40.478+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: hey_ud_dag.print_date scheduled__2025-02-01T23:00:00+00:00 [scheduled]>
[2025-02-03T05:22:40.482+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: hey_ud_dag.print_date scheduled__2025-02-01T23:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-03T05:22:40.484+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_date', run_id='scheduled__2025-02-01T23:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-03T05:22:40.486+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_date', 'scheduled__2025-02-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:22:41.395+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'hey_ud_dag', 'print_date', 'scheduled__2025-02-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/exampledag.py']
[2025-02-03T05:23:05.949+0000] {dagbag.py:588} INFO - Filling up the DagBag from /mnt/c/Users/Admin/Desktop/ud_data/Dataslush_umang/airflow/dags/exampledag.py
[2025-02-03T05:23:06.517+0000] {task_command.py:467} INFO - Running <TaskInstance: hey_ud_dag.print_date scheduled__2025-02-01T23:00:00+00:00 [queued]> on host DESKTOP-ENFJMA0.
[2025-02-03T05:23:11.308+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='hey_ud_dag', task_id='print_date', run_id='scheduled__2025-02-01T23:00:00+00:00', try_number=1, map_index=-1)
[2025-02-03T05:23:11.314+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=hey_ud_dag, task_id=print_date, run_id=scheduled__2025-02-01T23:00:00+00:00, map_index=-1, run_start_date=2025-02-03 05:23:10.163327+00:00, run_end_date=2025-02-03 05:23:10.840411+00:00, run_duration=0.677084, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-03 05:22:40.481279+00:00, queued_by_job_id=27, pid=8083
[2025-02-03T05:23:11.351+0000] {job.py:229} INFO - Heartbeat recovered after 38.51 seconds
[2025-02-03T05:23:14.395+0000] {dag.py:4180} INFO - Setting next_dagrun for weather_etl_pipeline to 2025-02-03 00:00:00+00:00, run_after=2025-02-04 00:00:00+00:00
[2025-02-03T05:23:14.442+0000] {dagrun.py:854} INFO - Marking run <DagRun hey_ud_dag @ 2025-02-01 23:00:00+00:00: scheduled__2025-02-01T23:00:00+00:00, state:running, queued_at: 2025-02-03 05:20:02.991343+00:00. externally triggered: False> successful
[2025-02-03T05:23:14.445+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=hey_ud_dag, execution_date=2025-02-01 23:00:00+00:00, run_id=scheduled__2025-02-01T23:00:00+00:00, run_start_date=2025-02-03 05:20:03.047784+00:00, run_end_date=2025-02-03 05:23:14.445941+00:00, run_duration=191.398157, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-01 23:00:00+00:00, data_interval_end=2025-02-02 23:00:00+00:00, dag_hash=ebaf72a0321aa3db83f0773665041a4e
[2025-02-03T05:23:14.451+0000] {dag.py:4180} INFO - Setting next_dagrun for hey_ud_dag to 2025-02-02 23:00:00+00:00, run_after=2025-02-03 23:00:00+00:00
[2025-02-03T05:23:14.456+0000] {dagrun.py:823} ERROR - Marking run <DagRun weather_etl_pipeline @ 2025-02-02 00:00:00+00:00: scheduled__2025-02-02T00:00:00+00:00, state:running, queued_at: 2025-02-03 05:20:02.973448+00:00. externally triggered: False> failed
[2025-02-03T05:23:14.459+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=weather_etl_pipeline, execution_date=2025-02-02 00:00:00+00:00, run_id=scheduled__2025-02-02T00:00:00+00:00, run_start_date=2025-02-03 05:20:03.049309+00:00, run_end_date=2025-02-03 05:23:14.459117+00:00, run_duration=191.409808, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-02 00:00:00+00:00, data_interval_end=2025-02-03 00:00:00+00:00, dag_hash=177d7a241483b43fcf3596b1a4912f6b
[2025-02-03T05:23:14.464+0000] {dag.py:4180} INFO - Setting next_dagrun for weather_etl_pipeline to 2025-02-03 00:00:00+00:00, run_after=2025-02-04 00:00:00+00:00
[2025-02-03T05:24:01.989+0000] {job.py:229} INFO - Heartbeat recovered after 41.92 seconds
[2025-02-03T05:25:22.147+0000] {job.py:229} INFO - Heartbeat recovered after 45.42 seconds
[2025-02-03T05:27:34.091+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:30:56.730+0000] {job.py:229} INFO - Heartbeat recovered after 52.80 seconds
[2025-02-03T05:32:39.739+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:35:32.954+0000] {job.py:229} INFO - Heartbeat recovered after 41.77 seconds
[2025-02-03T05:36:31.542+0000] {job.py:229} INFO - Heartbeat recovered after 45.62 seconds
[2025-02-03T05:37:43.238+0000] {job.py:229} INFO - Heartbeat recovered after 35.02 seconds
[2025-02-03T05:37:39.641+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:39:36.513+0000] {job.py:229} INFO - Heartbeat recovered after 48.82 seconds
[2025-02-03T05:41:28.542+0000] {job.py:229} INFO - Heartbeat recovered after 38.92 seconds
[2025-02-03T05:42:43.060+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:43:28.692+0000] {job.py:229} INFO - Heartbeat recovered after 41.78 seconds
[2025-02-03T05:43:33.795+0000] {job.py:229} INFO - Heartbeat recovered after 41.85 seconds
[2025-02-03T05:43:38.743+0000] {job.py:229} INFO - Heartbeat recovered after 41.76 seconds
[2025-02-03T05:44:24.684+0000] {job.py:229} INFO - Heartbeat recovered after 47.97 seconds
[2025-02-03T05:47:04.521+0000] {job.py:229} INFO - Heartbeat recovered after 44.49 seconds
[2025-02-03T05:47:34.726+0000] {job.py:229} INFO - Heartbeat recovered after 30.34 seconds
[2025-02-03T05:47:46.358+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:49:29.887+0000] {job.py:229} INFO - Heartbeat recovered after 38.28 seconds
[2025-02-03T05:50:09.377+0000] {job.py:229} INFO - Heartbeat recovered after 39.67 seconds
[2025-02-03T05:51:19.593+0000] {job.py:229} INFO - Heartbeat recovered after 39.60 seconds
[2025-02-03T05:53:20.104+0000] {job.py:229} INFO - Heartbeat recovered after 39.46 seconds
[2025-02-03T05:52:49.141+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:54:20.124+0000] {job.py:229} INFO - Heartbeat recovered after 54.53 seconds
[2025-02-03T05:56:20.351+0000] {job.py:229} INFO - Heartbeat recovered after 54.67 seconds
[2025-02-03T05:57:00.138+0000] {job.py:229} INFO - Heartbeat recovered after 39.94 seconds
[2025-02-03T05:58:10.809+0000] {job.py:229} INFO - Heartbeat recovered after 40.67 seconds
[2025-02-03T05:57:59.857+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-03T05:59:30.438+0000] {job.py:229} INFO - Heartbeat recovered after 44.00 seconds
[2025-02-03T06:00:55.556+0000] {job.py:229} INFO - Heartbeat recovered after 30.10 seconds
