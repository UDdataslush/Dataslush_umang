[2025-02-05T12:40:42.525+0530] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2025-02-05T12:40:42.561+0530] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-02-05T12:40:42.562+0530] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-02-05T12:40:42.568+0530] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 283288
[2025-02-05T12:40:42.570+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T12:40:42.572+0530] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-05T12:40:42.597+0530] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
Dag run  in running state
Dag information Queued at: 2025-02-05 07:11:19.280115+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T12:41:20.688+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
[2025-02-05T12:41:20.689+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T12:41:20.689+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
[2025-02-05T12:41:20.691+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T12:41:20.692+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T12:41:20.692+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:41:20.699+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:41:22.219+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T12:41:22.480+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T12:41:22.480+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:41:22.501+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:41:22.519+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:41:22.535+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T12:41:22.633+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:41:22.668+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T12:41:30.095+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=1, map_index=-1)
[2025-02-05T12:41:30.102+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:11:19.267322+00:00, map_index=-1, run_start_date=2025-02-05 07:11:22.712656+00:00, run_end_date=2025-02-05 07:11:29.556950+00:00, run_duration=6.844294, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:11:20.690488+00:00, queued_by_job_id=46, pid=283557
[2025-02-05T12:45:42.762+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T12:45:42.764+0530] {scheduler_job_runner.py:1972} INFO - Marked 1 SchedulerJob instances as failed
[2025-02-05T12:46:29.944+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
[2025-02-05T12:46:29.945+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T12:46:29.945+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>
[2025-02-05T12:46:29.948+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T12:46:29.949+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T12:46:29.950+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:46:29.956+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:11:19.267322+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:46:31.728+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T12:46:31.988+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T12:46:31.989+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:46:32.010+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:46:32.028+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:46:32.044+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T12:46:32.143+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:46:32.179+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:11:19.267322+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T12:46:33.624+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:11:19.267322+00:00', try_number=2, map_index=-1)
[2025-02-05T12:46:33.628+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:11:19.267322+00:00, map_index=-1, run_start_date=2025-02-05 07:16:32.223417+00:00, run_end_date=2025-02-05 07:16:33.085220+00:00, run_duration=0.861803, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:16:29.947006+00:00, queued_by_job_id=46, pid=285774
[2025-02-05T12:46:33.835+0530] {dagrun.py:823} ERROR - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 07:11:19.267322+00:00: manual__2025-02-05T07:11:19.267322+00:00, state:running, queued_at: 2025-02-05 07:11:19.280115+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:brewery_data_pipeline Run id: manual__2025-02-05T07:11:19.267322+00:00 external trigger: True
Failed with message: task_failure
[2025-02-05T12:46:33.836+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 07:11:19.267322+00:00, run_id=manual__2025-02-05T07:11:19.267322+00:00, run_start_date=2025-02-05 07:11:20.652095+00:00, run_end_date=2025-02-05 07:16:33.836158+00:00, run_duration=313.184063, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
[2025-02-05T12:50:42.911+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 07:22:58.448980+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T12:52:59.896+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
[2025-02-05T12:52:59.896+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T12:52:59.897+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
[2025-02-05T12:52:59.898+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T12:52:59.899+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T12:52:59.899+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:52:59.906+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:53:01.539+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T12:53:01.823+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T12:53:01.824+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:53:01.847+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:53:01.874+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:53:01.894+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T12:53:01.999+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:53:02.033+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T12:53:22.864+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=1, map_index=-1)
[2025-02-05T12:53:22.868+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:22:58.421946+00:00, map_index=-1, run_start_date=2025-02-05 07:23:02.079515+00:00, run_end_date=2025-02-05 07:23:22.256967+00:00, run_duration=20.177452, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:22:59.897843+00:00, queued_by_job_id=46, pid=286850
[2025-02-05T12:55:43.205+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T12:58:22.631+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
[2025-02-05T12:58:22.631+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T12:58:22.631+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>
[2025-02-05T12:58:22.633+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T12:58:22.633+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T12:58:22.634+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:58:22.640+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:22:58.421946+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T12:58:24.242+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T12:58:24.515+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T12:58:24.516+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:58:24.537+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T12:58:24.557+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:58:24.572+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T12:58:24.672+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T12:58:24.715+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:22:58.421946+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T12:58:36.285+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:22:58.421946+00:00', try_number=2, map_index=-1)
[2025-02-05T12:58:36.289+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:22:58.421946+00:00, map_index=-1, run_start_date=2025-02-05 07:28:24.759646+00:00, run_end_date=2025-02-05 07:28:35.709375+00:00, run_duration=10.949729, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:28:22.632493+00:00, queued_by_job_id=46, pid=289022
[2025-02-05T12:58:36.583+0530] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 07:22:58.421946+00:00: manual__2025-02-05T07:22:58.421946+00:00, state:running, queued_at: 2025-02-05 07:22:58.448980+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 07:22:59.844597+00:00 end:2025-02-05 07:28:36.583946+00:00
[2025-02-05T12:58:36.584+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 07:22:58.421946+00:00, run_id=manual__2025-02-05T07:22:58.421946+00:00, run_start_date=2025-02-05 07:22:59.844597+00:00, run_end_date=2025-02-05 07:28:36.583946+00:00, run_duration=336.739349, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
[2025-02-05T13:00:43.371+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:05:43.517+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:10:43.591+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:15:43.733+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:20:43.876+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:25:44.177+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 07:57:56.331138+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T13:27:57.607+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [scheduled]>
[2025-02-05T13:27:57.608+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T13:27:57.608+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [scheduled]>
[2025-02-05T13:27:57.609+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T13:27:57.610+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:57:56.302963+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T13:27:57.610+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:57:56.302963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T13:27:57.616+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T07:57:56.302963+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T13:27:59.110+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T13:27:59.367+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T13:27:59.367+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T13:27:59.388+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T13:27:59.408+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T13:27:59.424+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T13:27:59.524+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T13:27:59.559+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T07:57:56.302963+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T13:28:01.235+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T07:57:56.302963+00:00', try_number=1, map_index=-1)
[2025-02-05T13:28:01.239+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T07:57:56.302963+00:00, map_index=-1, run_start_date=2025-02-05 07:57:59.612637+00:00, run_end_date=2025-02-05 07:58:00.689153+00:00, run_duration=1.076516, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 07:57:57.608887+00:00, queued_by_job_id=46, pid=291318
[2025-02-05T13:28:01.494+0530] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 07:57:56.302963+00:00: manual__2025-02-05T07:57:56.302963+00:00, state:running, queued_at: 2025-02-05 07:57:56.331138+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 07:57:57.583570+00:00 end:2025-02-05 07:58:01.495144+00:00
[2025-02-05T13:28:01.495+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 07:57:56.302963+00:00, run_id=manual__2025-02-05T07:57:56.302963+00:00, run_start_date=2025-02-05 07:57:57.583570+00:00, run_end_date=2025-02-05 07:58:01.495144+00:00, run_duration=3.911574, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
[2025-02-05T13:30:44.329+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:35:44.446+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:40:44.594+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:45:44.743+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:50:44.922+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T13:55:45.039+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:00:45.185+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:05:45.243+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 08:40:23.634217+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:10:24.241+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [scheduled]>
[2025-02-05T14:10:24.242+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T14:10:24.242+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [scheduled]>
[2025-02-05T14:10:24.244+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T14:10:24.244+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:40:23.623285+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T14:10:24.245+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:40:23.623285+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:10:24.251+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:40:23.623285+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:10:25.866+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T14:10:26.148+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T14:10:26.148+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:10:26.170+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:10:26.190+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:10:26.206+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T14:10:26.307+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:10:26.343+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:40:23.623285+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T14:10:33.841+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:40:23.623285+00:00', try_number=1, map_index=-1)
[2025-02-05T14:10:33.845+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:40:23.623285+00:00, map_index=-1, run_start_date=2025-02-05 08:40:26.389205+00:00, run_end_date=2025-02-05 08:40:33.281916+00:00, run_duration=6.892711, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:40:24.243297+00:00, queued_by_job_id=46, pid=294856
[2025-02-05T14:10:34.033+0530] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:40:23.623285+00:00: manual__2025-02-05T08:40:23.623285+00:00, state:running, queued_at: 2025-02-05 08:40:23.634217+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 08:40:24.213473+00:00 end:2025-02-05 08:40:34.033995+00:00
[2025-02-05T14:10:34.034+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:40:23.623285+00:00, run_id=manual__2025-02-05T08:40:23.623285+00:00, run_start_date=2025-02-05 08:40:24.213473+00:00, run_end_date=2025-02-05 08:40:34.033995+00:00, run_duration=9.820522, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:10:45.381+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:15:45.619+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:20:45.767+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 08:52:55.945062+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:22:56.209+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [scheduled]>
[2025-02-05T14:22:56.209+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T14:22:56.209+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [scheduled]>
[2025-02-05T14:22:56.211+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T14:22:56.212+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:52:55+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T14:22:56.212+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:52:55+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:22:56.219+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:52:55+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:22:57.587+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T14:22:57.816+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T14:22:57.817+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:22:57.835+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:22:57.852+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:22:57.866+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T14:22:57.957+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:22:57.988+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:52:55+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T14:22:59.290+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:52:55+00:00', try_number=1, map_index=-1)
[2025-02-05T14:22:59.293+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:52:55+00:00, map_index=-1, run_start_date=2025-02-05 08:52:58.031574+00:00, run_end_date=2025-02-05 08:52:58.799696+00:00, run_duration=0.768122, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:52:56.210597+00:00, queued_by_job_id=46, pid=296434
[2025-02-05T14:22:59.570+0530] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:52:55+00:00: manual__2025-02-05T08:52:55+00:00, state:running, queued_at: 2025-02-05 08:52:55.945062+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 08:52:56.177994+00:00 end:2025-02-05 08:52:59.570554+00:00
[2025-02-05T14:22:59.570+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:52:55+00:00, run_id=manual__2025-02-05T08:52:55+00:00, run_start_date=2025-02-05 08:52:56.177994+00:00, run_end_date=2025-02-05 08:52:59.570554+00:00, run_duration=3.39256, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:25:46.000+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 08:57:19.539313+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:27:20.434+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [scheduled]>
[2025-02-05T14:27:20.434+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T14:27:20.435+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [scheduled]>
[2025-02-05T14:27:20.436+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T14:27:20.436+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:57:19.525810+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T14:27:20.437+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:57:19.525810+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:27:20.443+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:57:19.525810+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:27:21.932+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T14:27:22.186+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T14:27:22.186+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:27:22.207+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:27:22.225+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:27:22.241+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T14:27:22.342+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:27:22.376+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:57:19.525810+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T14:27:33.993+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:57:19.525810+00:00', try_number=1, map_index=-1)
[2025-02-05T14:27:33.998+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:57:19.525810+00:00, map_index=-1, run_start_date=2025-02-05 08:57:22.419763+00:00, run_end_date=2025-02-05 08:57:33.452857+00:00, run_duration=11.033094, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:57:20.435800+00:00, queued_by_job_id=46, pid=297042
[2025-02-05T14:27:34.188+0530] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:57:19.525810+00:00: manual__2025-02-05T08:57:19.525810+00:00, state:running, queued_at: 2025-02-05 08:57:19.539313+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 08:57:20.411069+00:00 end:2025-02-05 08:57:34.188864+00:00
[2025-02-05T14:27:34.188+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:57:19.525810+00:00, run_id=manual__2025-02-05T08:57:19.525810+00:00, run_start_date=2025-02-05 08:57:20.411069+00:00, run_end_date=2025-02-05 08:57:34.188864+00:00, run_duration=13.777795, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
Dag run  in running state
Dag information Queued at: 2025-02-05 08:58:09.774758+00:00 hash info: b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:28:10.318+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [scheduled]>
[2025-02-05T14:28:10.319+0530] {scheduler_job_runner.py:507} INFO - DAG brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T14:28:10.319+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [scheduled]>
[2025-02-05T14:28:10.320+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T14:28:10.321+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:58:09.766391+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T14:28:10.321+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:58:09.766391+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:28:10.327+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'brewery_data_pipeline', 'fetch_store_brewery_data', 'manual__2025-02-05T08:58:09.766391+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T14:28:11.844+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T14:28:12.095+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T14:28:12.096+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:28:12.118+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T14:28:12.137+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:28:12.153+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T14:28:12.250+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T14:28:12.282+0530] {task_command.py:467} INFO - Running <TaskInstance: brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-05T08:58:09.766391+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T14:28:13.781+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='brewery_data_pipeline', task_id='fetch_store_brewery_data', run_id='manual__2025-02-05T08:58:09.766391+00:00', try_number=1, map_index=-1)
[2025-02-05T14:28:13.785+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=brewery_data_pipeline, task_id=fetch_store_brewery_data, run_id=manual__2025-02-05T08:58:09.766391+00:00, map_index=-1, run_start_date=2025-02-05 08:58:12.325770+00:00, run_end_date=2025-02-05 08:58:13.240928+00:00, run_duration=0.915158, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 08:58:10.320032+00:00, queued_by_job_id=46, pid=297243
[2025-02-05T14:28:13.989+0530] {dagrun.py:854} INFO - Marking run <DagRun brewery_data_pipeline @ 2025-02-05 08:58:09.766391+00:00: manual__2025-02-05T08:58:09.766391+00:00, state:running, queued_at: 2025-02-05 08:58:09.774758+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 08:58:10.293335+00:00 end:2025-02-05 08:58:13.989542+00:00
[2025-02-05T14:28:13.989+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=brewery_data_pipeline, execution_date=2025-02-05 08:58:09.766391+00:00, run_id=manual__2025-02-05T08:58:09.766391+00:00, run_start_date=2025-02-05 08:58:10.293335+00:00, run_end_date=2025-02-05 08:58:13.989542+00:00, run_duration=3.696207, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=b6af88a73c655a6c118d39a9576ae703
[2025-02-05T14:30:46.166+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:35:46.407+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:40:46.564+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:45:46.865+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:50:47.150+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T14:55:47.320+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:00:50.629+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:05:50.883+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:06:24.582+0530] {manager.py:537} INFO - DAG brewery_data_pipeline is missing and will be deactivated.
[2025-02-05T15:06:24.594+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-05T15:06:24.603+0530] {manager.py:553} INFO - Deleted DAG brewery_data_pipeline in serialized_dag table
[2025-02-05T15:10:51.047+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:15:51.345+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:20:52.128+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:25:52.172+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:30:52.373+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:35:52.519+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:40:52.708+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:45:52.886+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:50:53.740+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T15:55:53.887+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:00:54.049+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:05:54.802+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:10:40.680+0530] {dag.py:4180} INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-05 00:00:00+00:00, run_after=2025-02-06 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-05 10:40:40.671494+00:00 hash info: 3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-05T16:10:40.706+0530] {dagrun.py:854} INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 00:00:00+00:00: scheduled__2025-02-04T00:00:00+00:00, state:running, queued_at: 2025-02-05 10:40:40.671494+00:00. externally triggered: False> successful
Dag run in success state
Dag run start:2025-02-05 10:40:40.694390+00:00 end:2025-02-05 10:40:40.707042+00:00
[2025-02-05T16:10:40.707+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 00:00:00+00:00, run_id=scheduled__2025-02-04T00:00:00+00:00, run_start_date=2025-02-05 10:40:40.694390+00:00, run_end_date=2025-02-05 10:40:40.707042+00:00, run_duration=0.012652, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-05T16:10:40.710+0530] {dag.py:4180} INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-05 00:00:00+00:00, run_after=2025-02-06 00:00:00+00:00
[2025-02-05T16:10:40.713+0530] {dagrun.py:977} ERROR - Failed to get task for ti <TaskInstance: A_brewery_data_pipeline.fetch_store_brewery_data manual__2025-02-04T07:13:04.631466+00:00 [up_for_retry]>. Marking it as removed.
[2025-02-05T16:10:40.716+0530] {dagrun.py:854} INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-04 07:13:04.631466+00:00: manual__2025-02-04T07:13:04.631466+00:00, state:running, queued_at: 2025-02-04 07:13:04.653327+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-04 07:13:06.122188+00:00 end:2025-02-05 10:40:40.716437+00:00
[2025-02-05T16:10:40.716+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-04 07:13:04.631466+00:00, run_id=manual__2025-02-04T07:13:04.631466+00:00, run_start_date=2025-02-04 07:13:06.122188+00:00, run_end_date=2025-02-05 10:40:40.716437+00:00, run_duration=98854.594249, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-03 07:13:04.631466+00:00, data_interval_end=2025-02-04 07:13:04.631466+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-05T16:10:54.970+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:15:55.161+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:19:04.343+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-05T16:19:04.351+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-05T16:19:04.363+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-05T16:20:55.327+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:25:55.370+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:30:55.520+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:35:55.680+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:40:55.968+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:45:56.117+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:50:56.401+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T16:55:56.569+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 11:30:10.113844+00:00 hash info: 3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-05T17:00:11.381+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
[2025-02-05T17:00:11.382+0530] {scheduler_job_runner.py:507} INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T17:00:11.382+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
[2025-02-05T17:00:11.383+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T17:00:11.384+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T17:00:11.384+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T17:00:11.390+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T17:00:12.874+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T17:00:13.131+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T17:00:13.131+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T17:00:13.154+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T17:00:13.172+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T17:00:13.187+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T17:00:13.283+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T17:00:13.316+0530] {task_command.py:467} INFO - Running <TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T17:00:14.709+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=1, map_index=-1)
[2025-02-05T17:00:14.713+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_and_store_brewery_data, run_id=manual__2025-02-05T11:30:10.100554+00:00, map_index=-1, run_start_date=2025-02-05 11:30:13.359348+00:00, run_end_date=2025-02-05 11:30:14.147943+00:00, run_duration=0.788595, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 11:30:11.382824+00:00, queued_by_job_id=46, pid=313612
[2025-02-05T17:00:56.727+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:05:14.673+0530] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
[2025-02-05T17:05:14.673+0530] {scheduler_job_runner.py:507} INFO - DAG A_brewery_data_pipeline has 0/16 running and queued tasks
[2025-02-05T17:05:14.673+0530] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>
[2025-02-05T17:05:14.675+0530] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-05T17:05:14.675+0530] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-05T17:05:14.675+0530] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T17:05:14.681+0530] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'A_brewery_data_pipeline', 'fetch_and_store_brewery_data', 'manual__2025-02-05T11:30:10.100554+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline_miniproject.py']
[2025-02-05T17:05:16.066+0530] {dagbag.py:588} INFO - Filling up the DagBag from /home/umang/airflow/dags/pipeline_miniproject.py
[2025-02-05T17:05:16.293+0530] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/umang/Dataslush_umang/airflow_env/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-05T17:05:16.294+0530] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T17:05:16.313+0530] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-05T17:05:16.331+0530] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T17:05:16.345+0530] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-05T17:05:16.443+0530] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-05T17:05:16.476+0530] {task_command.py:467} INFO - Running <TaskInstance: A_brewery_data_pipeline.fetch_and_store_brewery_data manual__2025-02-05T11:30:10.100554+00:00 [queued]> on host umang-Latitude-7490
[2025-02-05T17:05:17.871+0530] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='A_brewery_data_pipeline', task_id='fetch_and_store_brewery_data', run_id='manual__2025-02-05T11:30:10.100554+00:00', try_number=2, map_index=-1)
[2025-02-05T17:05:17.875+0530] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=A_brewery_data_pipeline, task_id=fetch_and_store_brewery_data, run_id=manual__2025-02-05T11:30:10.100554+00:00, map_index=-1, run_start_date=2025-02-05 11:35:16.518485+00:00, run_end_date=2025-02-05 11:35:17.369896+00:00, run_duration=0.851411, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-05 11:35:14.674354+00:00, queued_by_job_id=46, pid=315825
[2025-02-05T17:05:18.080+0530] {dagrun.py:823} ERROR - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-05 11:30:10.100554+00:00: manual__2025-02-05T11:30:10.100554+00:00, state:running, queued_at: 2025-02-05 11:30:10.113844+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:A_brewery_data_pipeline Run id: manual__2025-02-05T11:30:10.100554+00:00 external trigger: True
Failed with message: task_failure
[2025-02-05T17:05:18.081+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-05 11:30:10.100554+00:00, run_id=manual__2025-02-05T11:30:10.100554+00:00, run_start_date=2025-02-05 11:30:11.352145+00:00, run_end_date=2025-02-05 11:35:18.081246+00:00, run_duration=306.729101, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=684996f830e62e0e542494cfaa1b66e4
[2025-02-05T17:05:56.834+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-05 11:38:55.222285+00:00 hash info: 3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-05T17:08:56.426+0530] {dagrun.py:854} INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-05 11:38:55.209497+00:00: manual__2025-02-05T11:38:55.209497+00:00, state:running, queued_at: 2025-02-05 11:38:55.222285+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-05 11:38:56.414475+00:00 end:2025-02-05 11:38:56.427353+00:00
[2025-02-05T17:08:56.427+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-05 11:38:55.209497+00:00, run_id=manual__2025-02-05T11:38:55.209497+00:00, run_start_date=2025-02-05 11:38:56.414475+00:00, run_end_date=2025-02-05 11:38:56.427353+00:00, run_duration=0.012878, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-04 00:00:00+00:00, data_interval_end=2025-02-05 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-05T17:10:57.031+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:15:58.031+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:20:58.891+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:25:59.176+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:30:59.304+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:35:59.974+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:41:00.148+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:46:00.296+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:51:14.137+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T17:51:15.167+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-05T17:51:15.173+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-05T17:51:15.181+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-05T17:56:14.312+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T18:01:14.518+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T18:06:20.875+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T18:11:21.056+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T18:16:21.235+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-05T18:21:21.307+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:25:44.439+0530] {dag.py:4180} INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-06 00:00:00+00:00, run_after=2025-02-07 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-06 04:55:44.433870+00:00 hash info: 3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-06T10:25:44.478+0530] {dagrun.py:854} INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-05 00:00:00+00:00: scheduled__2025-02-05T00:00:00+00:00, state:running, queued_at: 2025-02-06 04:55:44.433870+00:00. externally triggered: False> successful
Dag run in success state
Dag run start:2025-02-06 04:55:44.456327+00:00 end:2025-02-06 04:55:44.478694+00:00
[2025-02-06T10:25:44.478+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-05 00:00:00+00:00, run_id=scheduled__2025-02-05T00:00:00+00:00, run_start_date=2025-02-06 04:55:44.456327+00:00, run_end_date=2025-02-06 04:55:44.478694+00:00, run_duration=0.022367, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-05 00:00:00+00:00, data_interval_end=2025-02-06 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-06T10:25:44.484+0530] {dag.py:4180} INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-06 00:00:00+00:00, run_after=2025-02-07 00:00:00+00:00
[2025-02-06T10:25:44.532+0530] {job.py:229} INFO - Heartbeat recovered after 57621.56 seconds
[2025-02-06T10:26:39.570+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:31:39.741+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:36:39.902+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:41:40.057+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:46:40.204+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:51:40.351+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T10:56:40.419+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:01:40.502+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:06:40.647+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:11:40.945+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:16:41.196+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:21:41.364+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:26:41.659+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:31:41.812+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:36:42.005+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:41:45.874+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:46:46.020+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:51:46.321+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T11:56:47.939+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:01:50.527+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:06:50.675+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:11:50.834+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:16:50.982+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:21:51.176+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:26:54.877+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:31:55.175+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:36:55.461+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:41:56.046+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:46:56.857+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:51:57.127+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T12:56:57.301+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:01:57.325+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:06:58.901+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:11:59.048+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:17:02.880+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:22:03.085+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:27:03.252+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:32:05.339+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:37:05.805+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:42:06.804+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:47:06.952+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:52:06.987+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T13:57:06.328+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:02:07.219+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:07:07.181+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:12:12.192+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:17:12.361+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:22:12.505+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:22:50.833+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-06T14:22:50.837+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-06T14:22:50.846+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-06T14:27:12.652+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:32:12.800+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:37:12.967+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:42:13.342+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:47:13.583+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:52:13.748+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T14:57:13.891+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:01:19.489+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-06T15:01:19.491+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-06T15:01:19.500+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-06T15:02:14.057+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:07:14.068+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:12:14.721+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:17:14.866+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:22:15.032+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:27:15.140+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:32:15.320+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:37:15.467+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:42:15.664+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:47:15.832+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:52:16.187+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:57:16.357+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T15:59:45.024+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-06T15:59:45.032+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-06T15:59:45.041+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-06T16:02:16.526+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:07:16.694+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:12:15.345+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:16:00.359+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-06T16:16:00.361+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-06T16:16:00.370+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-06T16:17:20.195+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:22:20.827+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:27:21.133+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:32:25.102+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:37:25.263+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:42:25.558+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:47:26.383+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:52:29.968+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T16:57:28.992+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:02:29.968+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:07:30.331+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:12:30.483+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:17:30.657+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:22:30.825+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:27:31.049+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:32:35.897+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:37:36.062+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:42:40.316+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:47:40.481+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:52:40.628+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T17:57:40.793+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T18:02:40.959+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T18:07:55.346+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T18:12:55.623+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-06T18:13:45.935+0530] {job.py:229} INFO - Heartbeat recovered after 33.21 seconds
[2025-02-06T18:14:10.864+0530] {manager.py:537} INFO - DAG A_brewery_data_pipeline is missing and will be deactivated.
[2025-02-06T18:14:10.866+0530] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-06T18:14:10.875+0530] {manager.py:553} INFO - Deleted DAG A_brewery_data_pipeline in serialized_dag table
[2025-02-06T18:17:55.792+0530] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-07T10:35:36.897+0530] {dag.py:4180} INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-07 00:00:00+00:00, run_after=2025-02-08 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-07 05:05:36.894472+00:00 hash info: 3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-07T10:35:36.920+0530] {dagrun.py:854} INFO - Marking run <DagRun A_brewery_data_pipeline @ 2025-02-06 00:00:00+00:00: scheduled__2025-02-06T00:00:00+00:00, state:running, queued_at: 2025-02-07 05:05:36.894472+00:00. externally triggered: False> successful
Dag run in success state
Dag run start:2025-02-07 05:05:36.908505+00:00 end:2025-02-07 05:05:36.921136+00:00
[2025-02-07T10:35:36.921+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=A_brewery_data_pipeline, execution_date=2025-02-06 00:00:00+00:00, run_id=scheduled__2025-02-06T00:00:00+00:00, run_start_date=2025-02-07 05:05:36.908505+00:00, run_end_date=2025-02-07 05:05:36.921136+00:00, run_duration=0.012631, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-06 00:00:00+00:00, data_interval_end=2025-02-07 00:00:00+00:00, dag_hash=3a1e39cfc9f7114b80fe31a98797eb0f
[2025-02-07T10:35:36.923+0530] {dag.py:4180} INFO - Setting next_dagrun for A_brewery_data_pipeline to 2025-02-07 00:00:00+00:00, run_after=2025-02-08 00:00:00+00:00
[2025-02-07T10:35:36.952+0530] {job.py:229} INFO - Heartbeat recovered after 58455.12 seconds
[2025-02-07T10:35:44.284+0530] {scheduler_job_runner.py:272} INFO - Exiting gracefully upon receiving signal 15
[2025-02-07T10:35:44.442+0530] {process_utils.py:132} INFO - Sending 15 to group 283288. PIDs of all processes in the group: []
[2025-02-07T10:35:44.443+0530] {process_utils.py:87} INFO - Sending the signal 15 to group 283288
[2025-02-07T10:35:44.443+0530] {process_utils.py:101} INFO - Sending the signal 15 to process 283288 as process group is missing.
[2025-02-07T10:35:44.444+0530] {process_utils.py:132} INFO - Sending 15 to group 283288. PIDs of all processes in the group: []
[2025-02-07T10:35:44.444+0530] {process_utils.py:87} INFO - Sending the signal 15 to group 283288
[2025-02-07T10:35:44.445+0530] {process_utils.py:101} INFO - Sending the signal 15 to process 283288 as process group is missing.
[2025-02-07T10:35:44.445+0530] {scheduler_job_runner.py:1029} INFO - Exited execute loop
